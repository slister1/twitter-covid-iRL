{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"conspiracy_classifier.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm","authorship_tag":"ABX9TyMcdSoHg3mNU08HDK1LFgyK"},"kernelspec":{"display_name":"Python 3","name":"python3"},"widgets":{"application/vnd.jupyter.widget-state+json":{"cd2623ec31a447d28fb8f797689d1dbc":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_14bf5565de5f415d9af2e95c28075302","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_a59821c2dc014f0fa90b471e939e84a8","IPY_MODEL_7e2ddf5cd25941b6a4ed097bf738f00f"]}},"14bf5565de5f415d9af2e95c28075302":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"a59821c2dc014f0fa90b471e939e84a8":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_b3d4bc4b26074f10a49212548a357d92","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":231508,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":231508,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_d575f1f240ce46fc93c72068724316ee"}},"7e2ddf5cd25941b6a4ed097bf738f00f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_d3a25fe1879540ffa62c727f1cb707d3","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"â€‹","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 232k/232k [00:01&lt;00:00, 213kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_336ac25fd0844a7b962262cbd7962a86"}},"b3d4bc4b26074f10a49212548a357d92":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"d575f1f240ce46fc93c72068724316ee":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"d3a25fe1879540ffa62c727f1cb707d3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"336ac25fd0844a7b962262cbd7962a86":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"55c899b0495c43c98cb205885a1b04ce":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_1511acb9d0514ceeac5376097e0f2d45","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_fdeaf801ccb04096aeba0f536b3fbb66","IPY_MODEL_44b914bc3ee14b3a8b644d64cfdb7cd7"]}},"1511acb9d0514ceeac5376097e0f2d45":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"fdeaf801ccb04096aeba0f536b3fbb66":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_cca4af504daf41fbad75e2b18b34e05b","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":28,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":28,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_2da816c37a9843fdbdc8d8442fce3378"}},"44b914bc3ee14b3a8b644d64cfdb7cd7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_5d9e5c0b146a4aafab330ab871661efc","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"â€‹","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 28.0/28.0 [00:00&lt;00:00, 35.9B/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_08d56da12c4647228c871673b7173216"}},"cca4af504daf41fbad75e2b18b34e05b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"2da816c37a9843fdbdc8d8442fce3378":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"5d9e5c0b146a4aafab330ab871661efc":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"08d56da12c4647228c871673b7173216":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"90427d65ad2641e0b60170e940c1edd1":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_5f9c3bb8cec144f69049d50189e4ee0b","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_8216382786fc45ccb90f99b261efaba3","IPY_MODEL_e2c2b88355ef4012a829ab5c84561454"]}},"5f9c3bb8cec144f69049d50189e4ee0b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"8216382786fc45ccb90f99b261efaba3":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_a612f557fe2647379411f1875f4f8c35","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":466062,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":466062,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_755a14b7e7c54dc8b9096b166c7ada1a"}},"e2c2b88355ef4012a829ab5c84561454":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_c65e737bb4bc4189bc1903e6c487c18f","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"â€‹","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 466k/466k [00:00&lt;00:00, 1.39MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_a8b200de22dd416db6f0b2cabe3ae1cf"}},"a612f557fe2647379411f1875f4f8c35":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"755a14b7e7c54dc8b9096b166c7ada1a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"c65e737bb4bc4189bc1903e6c487c18f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"a8b200de22dd416db6f0b2cabe3ae1cf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"debbbcf6ca0b435baf858fea935b6d47":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_9ec43d797b7649b88e188ab4b1cfcd06","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_0df424e2c65b40c883d7b70c443e0c32","IPY_MODEL_e84ee7d654c04a92875511e585eb0c22"]}},"9ec43d797b7649b88e188ab4b1cfcd06":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"0df424e2c65b40c883d7b70c443e0c32":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_84fc93af390c4979ba1d3c4b832131e5","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":570,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":570,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_32f1bc33307947e2ac3c96442bf92e82"}},"e84ee7d654c04a92875511e585eb0c22":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_d2aeb986a6bd4928ab3984e73bf9de95","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"â€‹","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 570/570 [01:50&lt;00:00, 5.16B/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_75c4d2743c0f48c4979f029780ecf634"}},"84fc93af390c4979ba1d3c4b832131e5":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"32f1bc33307947e2ac3c96442bf92e82":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"d2aeb986a6bd4928ab3984e73bf9de95":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"75c4d2743c0f48c4979f029780ecf634":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"c4ee15533a684272a0e55801d3fbe283":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_3272ca4fcc884f64b0462bc9f7622fdc","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_cac45abb0e6e4920b82640fecbb2363d","IPY_MODEL_23ffef066d5c4211b4bb1bfb058cbd95"]}},"3272ca4fcc884f64b0462bc9f7622fdc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"cac45abb0e6e4920b82640fecbb2363d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_b2268b8472484dcfbcbf2f8bcee6678e","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":440473133,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":440473133,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_d7378563e07d4edf832d2484c02804d5"}},"23ffef066d5c4211b4bb1bfb058cbd95":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_16dec479398a4fac9160d164bc17b9ce","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"â€‹","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 440M/440M [01:49&lt;00:00, 4.00MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_db8378c361424c12a2415504e9f8d999"}},"b2268b8472484dcfbcbf2f8bcee6678e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"d7378563e07d4edf832d2484c02804d5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"16dec479398a4fac9160d164bc17b9ce":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"db8378c361424c12a2415504e9f8d999":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"code","metadata":{"id":"kClgTTR2fGZs","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1621450085731,"user_tz":240,"elapsed":2708,"user":{"displayName":"Sydney Lister","photoUrl":"","userId":"17557786652639167110"}},"outputId":"51050ed2-ad5c-44d2-ca5e-76c4a151dfc7"},"source":["!pip install transformers"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.6.0)\n","Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.2)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (4.0.1)\n","Requirement already satisfied: huggingface-hub==0.0.8 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.8)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (8.0.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n","Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"OdkHWQwUe4-t","executionInfo":{"status":"ok","timestamp":1621448620288,"user_tz":240,"elapsed":4426,"user":{"displayName":"Sydney Lister","photoUrl":"","userId":"17557786652639167110"}}},"source":["import os \n","from google.colab import drive\n","import csv\n","import pandas as pd\n","import tensorflow as tf\n","import torch\n","import numpy as np\n","import time\n","import datetime\n","import random\n","from transformers import BertTokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","from sklearn.model_selection import train_test_split\n","from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","from transformers import BertForSequenceClassification, AdamW, BertConfig\n","from transformers import get_linear_schedule_with_warmup\n","from sklearn.dummy import DummyClassifier\n","# from torchtext.data import Iterator"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"aBA8lAbQkozD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1621450094376,"user_tz":240,"elapsed":3718,"user":{"displayName":"Sydney Lister","photoUrl":"","userId":"17557786652639167110"}},"outputId":"f2caffe5-d060-454a-8e64-13c8396c9daf"},"source":["# TO USE COLAB GPU FOR BERT TRAINING\n","# Get the GPU device name.\n","device_name = tf.test.gpu_device_name()\n","\n","# The device name should look like the following:\n","if device_name == '/device:GPU:0':\n","    print('Found GPU at: {}'.format(device_name))\n","else:\n","    raise SystemError('GPU device not found')"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Found GPU at: /device:GPU:0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"xpooBXlokyL5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1621450096897,"user_tz":240,"elapsed":1449,"user":{"displayName":"Sydney Lister","photoUrl":"","userId":"17557786652639167110"}},"outputId":"893f780b-5db8-4f54-f061-73707ae770ec"},"source":["# If there's a GPU available...\n","if torch.cuda.is_available():    \n","\n","    # Tell PyTorch to use the GPU.    \n","    device = torch.device(\"cuda\")\n","\n","    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n","\n","    print('We will use the GPU:', torch.cuda.get_device_name(0))\n","\n","# If not...\n","else:\n","    print('No GPU available, using the CPU instead.')\n","    device = torch.device(\"cpu\")"],"execution_count":5,"outputs":[{"output_type":"stream","text":["There are 1 GPU(s) available.\n","We will use the GPU: Tesla P100-PCIE-16GB\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9NJPS_PhihNi","executionInfo":{"status":"ok","timestamp":1621450171998,"user_tz":240,"elapsed":32633,"user":{"displayName":"Sydney Lister","photoUrl":"","userId":"17557786652639167110"}},"outputId":"6eae7c1d-72af-436e-8862-a2ee92792f36"},"source":["drive.mount('/content/gdrive', force_remount=True)\n","working_directory = '/gdrive/My Drive/ML Research ListerVosoughi/SMIP/Data'\n","os.chdir(os.getcwd() + working_directory)\n","dirpath = os.getcwd()"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eXrlt3YxLqrq","executionInfo":{"status":"ok","timestamp":1620921121555,"user_tz":240,"elapsed":28550,"user":{"displayName":"Sydney Lister","photoUrl":"","userId":"17557786652639167110"}},"outputId":"6535d6f0-bba1-4a1c-b7bf-3fbc0dc23be5"},"source":["print(dirpath)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/gdrive/My Drive/ML Research ListerVosoughi/SMIP/Data\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Z81zvO_GtMpr"},"source":["## **Training** "]},{"cell_type":"code","metadata":{"id":"bp1LKwrfjqSw"},"source":["filename = 'masklabels.tsv'\n","df = pd.read_csv(dirpath + '/' + filename, sep='\\t')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"O5vjn3QsMF5Y","executionInfo":{"status":"ok","timestamp":1621450179305,"user_tz":240,"elapsed":1979,"user":{"displayName":"Sydney Lister","photoUrl":"","userId":"17557786652639167110"}}},"source":["filename = 'masklabel1.tsv'\n","df = pd.read_csv(dirpath + '/' + filename, sep='\\t')"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Iyg2SYJ_UIxa","executionInfo":{"status":"ok","timestamp":1621450189052,"user_tz":240,"elapsed":476,"user":{"displayName":"Sydney Lister","photoUrl":"","userId":"17557786652639167110"}},"outputId":"01307fdb-978e-47f4-9419-00f89bd3e611"},"source":["len(df[df['mask_label'] == 0])\n","len(df[df['mask_label'] == 1])"],"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["13900"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"k54bZvXOUkGF","executionInfo":{"status":"ok","timestamp":1620855934717,"user_tz":240,"elapsed":618,"user":{"displayName":"Sydney Lister","photoUrl":"","userId":"17557786652639167110"}},"outputId":"24102c8c-532b-4790-8a86-798690ecf9b1"},"source":["temp = df[df['mask_label'] == 1]['text']\n","temp"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["6        How to Significantly Slow Coronavirus? (featur...\n","7        RT @TamaraBraun: When sharing supplies please ...\n","8        How to Significantly Slow Coronavirus? (featur...\n","9        RT @TamaraBraun: When sharing supplies please ...\n","10       RT @TaiwanBirding: #WearAMask, even crap ones ...\n","                               ...                        \n","14863    I think now almost everyone forgotten the fact...\n","14864    I think now almost everyone forgotten the fact...\n","14865    RT @buddy_21011: Whatâ€™s your name? #englishbul...\n","14866    RT @buddy_21011: Whatâ€™s your name? #englishbul...\n","14867    RT @buddy_21011: Whatâ€™s your name? #englishbul...\n","Name: text, Length: 13900, dtype: object"]},"metadata":{"tags":[]},"execution_count":48}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":297},"id":"snwzdC2Zkfod","executionInfo":{"status":"ok","timestamp":1621450505412,"user_tz":240,"elapsed":693,"user":{"displayName":"Sydney Lister","photoUrl":"","userId":"17557786652639167110"}},"outputId":"4ed37a10-b3df-4193-a65e-99f8729a14cd"},"source":["tweets = df.text.values\n","labels = df.mask_label.values\n","df.head()"],"execution_count":21,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Unnamed: 0</th>\n","      <th>hashtags</th>\n","      <th>user id</th>\n","      <th>text</th>\n","      <th>linked urls</th>\n","      <th>retweeted user id</th>\n","      <th>retweet_text</th>\n","      <th>time</th>\n","      <th>week</th>\n","      <th>mask_label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>[{'text': 'Straya', 'indices': [75, 82]}, {'te...</td>\n","      <td>1209981600173215700</td>\n","      <td>RT @CarlsCarla: Never fear, Aussies are wardin...</td>\n","      <td>[]</td>\n","      <td>24622413.0</td>\n","      <td>Never fear, Aussies are warding off Coronaviru...</td>\n","      <td>2020-04-03 08:52:30</td>\n","      <td>10.0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>[{'text': 'Straya', 'indices': [75, 82]}, {'te...</td>\n","      <td>965767890</td>\n","      <td>RT @CarlsCarla: Never fear, Aussies are wardin...</td>\n","      <td>[]</td>\n","      <td>24622413.0</td>\n","      <td>Never fear, Aussies are warding off Coronaviru...</td>\n","      <td>2020-03-03 11:04:32</td>\n","      <td>10.0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>[{'text': 'Straya', 'indices': [75, 82]}, {'te...</td>\n","      <td>812474688</td>\n","      <td>RT @CarlsCarla: Never fear, Aussies are wardin...</td>\n","      <td>[]</td>\n","      <td>24622413.0</td>\n","      <td>Never fear, Aussies are warding off Coronaviru...</td>\n","      <td>2020-03-03 11:23:41</td>\n","      <td>10.0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>[{'text': 'Straya', 'indices': [75, 82]}, {'te...</td>\n","      <td>337688338</td>\n","      <td>RT @CarlsCarla: Never fear, Aussies are wardin...</td>\n","      <td>[]</td>\n","      <td>24622413.0</td>\n","      <td>Never fear, Aussies are warding off Coronaviru...</td>\n","      <td>2020-03-03 11:14:35</td>\n","      <td>10.0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>[{'text': 'Straya', 'indices': [75, 82]}, {'te...</td>\n","      <td>1069102867686944800</td>\n","      <td>RT @CarlsCarla: Never fear, Aussies are wardin...</td>\n","      <td>[]</td>\n","      <td>24622413.0</td>\n","      <td>Never fear, Aussies are warding off Coronaviru...</td>\n","      <td>2020-03-03 14:55:13</td>\n","      <td>10.0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   Unnamed: 0  ... mask_label\n","0           0  ...          0\n","1           1  ...          0\n","2           2  ...          0\n","3           3  ...          0\n","4           4  ...          0\n","\n","[5 rows x 10 columns]"]},"metadata":{"tags":[]},"execution_count":21}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BiqD_p3cY7bQ","executionInfo":{"status":"ok","timestamp":1620921135725,"user_tz":240,"elapsed":1051,"user":{"displayName":"Sydney Lister","photoUrl":"","userId":"17557786652639167110"}},"outputId":"d5c7747f-f554-4b01-9a7f-c9101eebd1f2"},"source":["train, val, train_y, validation_y = train_test_split(tweets, labels, \n","                                                            random_state=2018,stratify=labels, test_size=0.1)\n","dummy_clf = DummyClassifier(strategy=\"stratified\")\n","dummy_clf.fit(train, train_y)\n"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["DummyClassifier(constant=None, random_state=None, strategy='stratified')"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MySWtopFZZY2","executionInfo":{"status":"ok","timestamp":1620921138036,"user_tz":240,"elapsed":433,"user":{"displayName":"Sydney Lister","photoUrl":"","userId":"17557786652639167110"}},"outputId":"337cce9a-ca74-4a94-c8a7-e067371e37e8"},"source":["dummy_clf.predict(val)\n","dummy_clf.score(val, validation_y)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.8702084734364492"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"code","metadata":{"id":"Qd3iMDmjXjtc"},"source":["train, val, train_y, validation_y = train_test_split(tweets, labels, \n","                                                            random_state=2018,stratify=labels, test_size=0.1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_BYao9ZWXsrf","executionInfo":{"status":"ok","timestamp":1620682297140,"user_tz":240,"elapsed":809,"user":{"displayName":"Sydney Lister","photoUrl":"","userId":"17557786652639167110"}},"outputId":"9532381e-ac97-4df6-a0ad-71ea8653e6d8"},"source":["train.view()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array(['Who needs a Master Chief Mask?\\n#WearAMask #HaloUnited  https://t.co/HgWdDxyBhy',\n","       'Wear a mask, limit your movement to essentials, be a super hero ðŸ˜· ðŸ˜· #WearAMaskSaveALife',\n","       'RT @OptimistsPrime: If Death Valley park rangers can #WearAMask in 129 degrees you can wear one in the damn grocery store. #COVID19 https:/â€¦',\n","       ...,\n","       '@NYGovCuomo I donâ€™t get it you, nor your fellow elected Democrats donâ€™t believe in this.  #WearAMask #coronavirus    \\n\\nWhy should anyone believe or follow you? https://t.co/f039lwvx4e',\n","       'RT @silv24: 6.40am - Mask number 1 - waiting for the tube on my way in to work #wearamask #COVID19 https://t.co/CvYPwkxari',\n","       'RT @shachar_baron: Israeli reporter @BarakRavid rocking the White House today w his Theodore Herzl mask #WearAMask'],\n","      dtype=object)"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MzO3FN43XzIS","executionInfo":{"status":"ok","timestamp":1620682348772,"user_tz":240,"elapsed":731,"user":{"displayName":"Sydney Lister","photoUrl":"","userId":"17557786652639167110"}},"outputId":"7e8eacf5-23eb-4e42-8ba4-ecf94634f35f"},"source":["val.view()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array(['Either someone just wants their nappy changed...of theyâ€™ve just heard people STILL arenâ€™t following Covid Gov Restrictions!! ðŸ˜¤\\n\\n...I feel ya buddy. \\n\\n#wearadamnmask #stayathome #mondaymood \\n\\n@VicGovtNews @VicGovDHHS https://t.co/lEP3cMlk63',\n","       'RT @thewatcher303: Always wear a mask in public!  #WearADamnMask #HomeDepot https://t.co/g1l35fHbYF',\n","       '@molmccann All everyone had to do was #WearAMask and social distance, but no, they knew better, so here we are now.  Thanks all you selfish people!ðŸ˜¡',\n","       ...,\n","       'This is not even a worst case scenario estimate. \\n\\n#WearAMask https://t.co/tsbT8OEFEc',\n","       'A new design from WallsOfFame. Follow for more. #digitalart #artistoninstagram #fanart #wallsoffame #digitalart #love #mask #COVID19 #health #wearamask \\nGet my art printed on awesome products. Support me at Redbubble #RBandME:  https://t.co/EEckqwbBy0 #findyourthing #redbubble',\n","       'RT @SidneyPowell1: OK, so #HerdImmunity is working.\\nDeaths are going down.\\nOverall deaths for the year are down!\\n#NoMasks \\nStop the nonsensâ€¦'],\n","      dtype=object)"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"code","metadata":{"id":"PPF40a2rlYab","colab":{"base_uri":"https://localhost:8080/","height":166,"referenced_widgets":["cd2623ec31a447d28fb8f797689d1dbc","14bf5565de5f415d9af2e95c28075302","a59821c2dc014f0fa90b471e939e84a8","7e2ddf5cd25941b6a4ed097bf738f00f","b3d4bc4b26074f10a49212548a357d92","d575f1f240ce46fc93c72068724316ee","d3a25fe1879540ffa62c727f1cb707d3","336ac25fd0844a7b962262cbd7962a86","55c899b0495c43c98cb205885a1b04ce","1511acb9d0514ceeac5376097e0f2d45","fdeaf801ccb04096aeba0f536b3fbb66","44b914bc3ee14b3a8b644d64cfdb7cd7","cca4af504daf41fbad75e2b18b34e05b","2da816c37a9843fdbdc8d8442fce3378","5d9e5c0b146a4aafab330ab871661efc","08d56da12c4647228c871673b7173216","90427d65ad2641e0b60170e940c1edd1","5f9c3bb8cec144f69049d50189e4ee0b","8216382786fc45ccb90f99b261efaba3","e2c2b88355ef4012a829ab5c84561454","a612f557fe2647379411f1875f4f8c35","755a14b7e7c54dc8b9096b166c7ada1a","c65e737bb4bc4189bc1903e6c487c18f","a8b200de22dd416db6f0b2cabe3ae1cf"]},"executionInfo":{"status":"ok","timestamp":1621450416804,"user_tz":240,"elapsed":6706,"user":{"displayName":"Sydney Lister","photoUrl":"","userId":"17557786652639167110"}},"outputId":"f85925e2-f6aa-40a7-9efd-d8745865a328"},"source":["tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"],"execution_count":19,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"cd2623ec31a447d28fb8f797689d1dbc","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descriptiâ€¦"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"55c899b0495c43c98cb205885a1b04ce","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=28.0, style=ProgressStyle(description_wâ€¦"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"90427d65ad2641e0b60170e940c1edd1","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=466062.0, style=ProgressStyle(descriptiâ€¦"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4ykm_tn9leIj","executionInfo":{"status":"ok","timestamp":1620855959252,"user_tz":240,"elapsed":1087,"user":{"displayName":"Sydney Lister","photoUrl":"","userId":"17557786652639167110"}},"outputId":"35cd944b-0690-472b-d1e7-21ce86ade4bb"},"source":["# Print the original tweet.\n","print(' Original: ', tweets[0])\n","\n","# Print the tweet split into tokens.\n","print('Tokenized: ', tokenizer.tokenize(tweets[0]))\n","\n","# Print the tweet mapped to token ids.\n","print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(tweets[0])))"],"execution_count":null,"outputs":[{"output_type":"stream","text":[" Original:  RT @CarlsCarla: Never fear, Aussies are warding off Coronavirus with 3ply. #Straya #Stockpiling #Panicbuying #ToiletPaper #NoMasksNowuccasâ€¦\n","Tokenized:  ['rt', '@', 'carl', '##sca', '##rl', '##a', ':', 'never', 'fear', ',', 'aus', '##sies', 'are', 'ward', '##ing', 'off', 'corona', '##virus', 'with', '3', '##ply', '.', '#', 'stray', '##a', '#', 'stock', '##pi', '##ling', '#', 'panic', '##bu', '##ying', '#', 'toilet', '##paper', '#', 'no', '##mas', '##ks', '##now', '##uc', '##cas', 'â€¦']\n","Token IDs:  [19387, 1030, 5529, 15782, 12190, 2050, 1024, 2196, 3571, 1010, 17151, 14625, 2024, 4829, 2075, 2125, 21887, 23350, 2007, 1017, 22086, 1012, 1001, 15926, 2050, 1001, 4518, 8197, 2989, 1001, 6634, 8569, 14147, 1001, 11848, 23298, 1001, 2053, 9335, 5705, 19779, 14194, 15671, 1529]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EUdvsSQ7likl","executionInfo":{"status":"ok","timestamp":1620855972353,"user_tz":240,"elapsed":12398,"user":{"displayName":"Sydney Lister","photoUrl":"","userId":"17557786652639167110"}},"outputId":"14a228b2-ec28-4db8-efc1-2af12aeff4c8"},"source":["# Tokenize all of the tweet and map the tokens to thier word IDs.\n","input_ids = []\n","\n","# For every tweet...\n","for tweet in tweets:\n","    # `encode` will:\n","    #   (1) Tokenize the tweet.\n","    #   (2) Prepend the `[CLS]` token to the start.\n","    #   (3) Append the `[SEP]` token to the end.\n","    #   (4) Map tokens to their IDs.\n","    encoded_tweet = tokenizer.encode(\n","                        tweet,                      # Tweet to encode.\n","                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n","\n","                        # This function also supports truncation and conversion\n","                        # to pytorch tensors, but we need to do padding, so we\n","                        # can't use these features :( .\n","                        #max_length = 128,          # Truncate all tweets.\n","                        #return_tensors = 'pt',     # Return pytorch tensors.\n","                   )\n","    \n","    # Add the encoded tweet to the list.\n","    input_ids.append(encoded_tweet)\n","\n","# Print tweet 0, now as a list of IDs.\n","print('Original: ', tweets[0])\n","print('Token IDs:', input_ids[0])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Original:  RT @CarlsCarla: Never fear, Aussies are warding off Coronavirus with 3ply. #Straya #Stockpiling #Panicbuying #ToiletPaper #NoMasksNowuccasâ€¦\n","Token IDs: [101, 19387, 1030, 5529, 15782, 12190, 2050, 1024, 2196, 3571, 1010, 17151, 14625, 2024, 4829, 2075, 2125, 21887, 23350, 2007, 1017, 22086, 1012, 1001, 15926, 2050, 1001, 4518, 8197, 2989, 1001, 6634, 8569, 14147, 1001, 11848, 23298, 1001, 2053, 9335, 5705, 19779, 14194, 15671, 1529, 102]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CeNRwmISll2u","executionInfo":{"elapsed":483,"status":"ok","timestamp":1611842717331,"user":{"displayName":"Sydney Lister","photoUrl":"","userId":"17557786652639167110"},"user_tz":300},"outputId":"f0ea89f9-db36-4ecb-ebcc-12275478ce11"},"source":["print('Max tweet length: ', max([len(tweet) for tweet in input_ids]))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Max tweet length:  104\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"g8Bx4T_ul4IW","executionInfo":{"status":"ok","timestamp":1620855972558,"user_tz":240,"elapsed":9198,"user":{"displayName":"Sydney Lister","photoUrl":"","userId":"17557786652639167110"}},"outputId":"2fba0291-9158-411a-dab3-10c8dd37d319"},"source":["# Set the maximum sequence length.\n","# I've chosen 400 somewhat arbitrarily. It's slightly larger than the\n","# maximum training tweet length of 357...\n","MAX_LEN = 400\n","\n","print('\\nPadding/truncating all sentences to %d values...' % MAX_LEN)\n","\n","print('\\nPadding token: \"{:}\", ID: {:}'.format(tokenizer.pad_token, tokenizer.pad_token_id))\n","\n","# Pad our input tokens with value 0.\n","# \"post\" indicates that we want to pad and truncate at the end of the sequence,\n","# as opposed to the beginning.\n","input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", \n","                          value=0, truncating=\"post\", padding=\"post\")\n","\n","print('\\nDone.')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\n","Padding/truncating all sentences to 400 values...\n","\n","Padding token: \"[PAD]\", ID: 0\n","\n","Done.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"jsTOWl22l7TE"},"source":["# Create attention masks\n","attention_masks = []\n","\n","# For each tweet...\n","for tweet in input_ids:\n","    \n","    # Create the attention mask.\n","    #   - If a token ID is 0, then it's padding, set the mask to 0.\n","    #   - If a token ID is > 0, then it's a real token, set the mask to 1.\n","    att_mask = [int(token_id > 0) for token_id in tweet]\n","    \n","    # Store the attention mask for this tweet.\n","    attention_masks.append(att_mask)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LTNWlsKVl9Vv"},"source":["# Use 90% for training and 10% for validation.\n","train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, \n","                                                             stratify=labels, test_size=0.1)\n","# Do the same for the masks.\n","train_masks, validation_masks, _, _ = train_test_split(attention_masks, labels,\n","                                             stratify=labels, test_size=0.1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"C52ZUQ1JWGDh","executionInfo":{"status":"ok","timestamp":1620686203972,"user_tz":240,"elapsed":686,"user":{"displayName":"Sydney Lister","photoUrl":"","userId":"17557786652639167110"}},"outputId":"15ccc7ce-14ac-4f26-a612-2698cf4c070b"},"source":["tl = validation_labels.tolist()\n","num0 = 0\n","for i in tl: \n","  if i == 1: num0+= 1 \n","num0\n","\n"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1390"]},"metadata":{"tags":[]},"execution_count":35}]},{"cell_type":"code","metadata":{"id":"3WYh0Z_DmF8_"},"source":["# Convert all inputs and labels into torch tensors, the required datatype \n","# for our model.\n","train_inputs = torch.tensor(train_inputs)\n","validation_inputs = torch.tensor(validation_inputs)\n","\n","train_labels = torch.tensor(train_labels)\n","validation_labels = torch.tensor(validation_labels)\n","\n","train_masks = torch.tensor(train_masks)\n","validation_masks = torch.tensor(validation_masks)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UYvku5pqmIHu"},"source":["# The DataLoader needs to know our batch size for training, so we specify it \n","# here.\n","# For fine-tuning BERT on a specific task, the authors recommend a batch size of\n","# 16 or 32.\n","\n","batch_size = 16\n","\n","# Create the DataLoader for our training set.\n","train_data = TensorDataset(train_inputs, train_masks, train_labels)\n","train_sampler = RandomSampler(train_data)\n","train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n","\n","# Create the DataLoader for our validation set.\n","validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n","validation_sampler = SequentialSampler(validation_data)\n","validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["debbbcf6ca0b435baf858fea935b6d47","9ec43d797b7649b88e188ab4b1cfcd06","0df424e2c65b40c883d7b70c443e0c32","e84ee7d654c04a92875511e585eb0c22","84fc93af390c4979ba1d3c4b832131e5","32f1bc33307947e2ac3c96442bf92e82","d2aeb986a6bd4928ab3984e73bf9de95","75c4d2743c0f48c4979f029780ecf634","c4ee15533a684272a0e55801d3fbe283","3272ca4fcc884f64b0462bc9f7622fdc","cac45abb0e6e4920b82640fecbb2363d","23ffef066d5c4211b4bb1bfb058cbd95","b2268b8472484dcfbcbf2f8bcee6678e","d7378563e07d4edf832d2484c02804d5","16dec479398a4fac9160d164bc17b9ce","db8378c361424c12a2415504e9f8d999"]},"id":"W7iioc4umVIt","executionInfo":{"status":"ok","timestamp":1621450238767,"user_tz":240,"elapsed":17375,"user":{"displayName":"Sydney Lister","photoUrl":"","userId":"17557786652639167110"}},"outputId":"6ef9e7c7-3de2-4dc3-e866-3bf208887344"},"source":["# Load BertForSequenceClassification, the pretrained BERT model with a single \n","# linear classification layer on top. \n","model = BertForSequenceClassification.from_pretrained(\n","    \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n","    num_labels = 2, # The number of output labels--2 for binary classification.\n","                    # You can increase this for multi-class tasks.   \n","    output_attentions = False, # Whether the model returns attentions weights.\n","    output_hidden_states = False, # Whether the model returns all hidden-states.\n",")\n","\n","# Tell pytorch to run this model on the GPU.\n","model.cuda()"],"execution_count":12,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"debbbcf6ca0b435baf858fea935b6d47","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=570.0, style=ProgressStyle(description_â€¦"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c4ee15533a684272a0e55801d3fbe283","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=440473133.0, style=ProgressStyle(descriâ€¦"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["BertForSequenceClassification(\n","  (bert): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (4): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (5): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (6): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (7): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (8): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (9): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (10): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (11): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): BertPooler(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n","  (dropout): Dropout(p=0.1, inplace=False)\n","  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",")"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"05IMWeKNvDyw","executionInfo":{"status":"ok","timestamp":1620855992866,"user_tz":240,"elapsed":408,"user":{"displayName":"Sydney Lister","photoUrl":"","userId":"17557786652639167110"}},"outputId":"5c2e4b4f-deda-4fa5-9a37-24c242d6e11e"},"source":["# Get all of the model's parameters as a list of tuples.\n","params = list(model.named_parameters())\n","\n","print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n","\n","print('==== Embedding Layer ====\\n')\n","\n","for p in params[0:5]:\n","    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n","\n","print('\\n==== First Transformer ====\\n')\n","\n","for p in params[5:21]:\n","    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n","\n","print('\\n==== Output Layer ====\\n')\n","\n","for p in params[-4:]:\n","    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["The BERT model has 201 different named parameters.\n","\n","==== Embedding Layer ====\n","\n","bert.embeddings.word_embeddings.weight                  (30522, 768)\n","bert.embeddings.position_embeddings.weight                (512, 768)\n","bert.embeddings.token_type_embeddings.weight                (2, 768)\n","bert.embeddings.LayerNorm.weight                              (768,)\n","bert.embeddings.LayerNorm.bias                                (768,)\n","\n","==== First Transformer ====\n","\n","bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n","bert.encoder.layer.0.attention.self.query.bias                (768,)\n","bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n","bert.encoder.layer.0.attention.self.key.bias                  (768,)\n","bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n","bert.encoder.layer.0.attention.self.value.bias                (768,)\n","bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n","bert.encoder.layer.0.attention.output.dense.bias              (768,)\n","bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n","bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n","bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n","bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n","bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n","bert.encoder.layer.0.output.dense.bias                        (768,)\n","bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n","bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n","\n","==== Output Layer ====\n","\n","bert.pooler.dense.weight                                  (768, 768)\n","bert.pooler.dense.bias                                        (768,)\n","classifier.weight                                           (2, 768)\n","classifier.bias                                                 (2,)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"eVfvyg4NvKZz","executionInfo":{"status":"ok","timestamp":1621450238768,"user_tz":240,"elapsed":10791,"user":{"displayName":"Sydney Lister","photoUrl":"","userId":"17557786652639167110"}}},"source":["# Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n","# I believe the 'W' stands for 'Weight Decay fix\"\n","optimizer = AdamW(model.parameters(),\n","                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n","                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n","                )\n"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"id":"EaV_AibTvQIC","colab":{"base_uri":"https://localhost:8080/","height":240},"executionInfo":{"status":"error","timestamp":1621450242848,"user_tz":240,"elapsed":444,"user":{"displayName":"Sydney Lister","photoUrl":"","userId":"17557786652639167110"}},"outputId":"bb2a5798-3f5c-4593-fe88-7a7ef0174afc"},"source":["# Number of training epochs (authors recommend between 2 and 4)\n","epochs = 4\n","\n","# Total number of training steps is number of batches * number of epochs.\n","total_steps = len(train_dataloader) * epochs\n","\n","# Create the learning rate scheduler.\n","scheduler = get_linear_schedule_with_warmup(optimizer, \n","                                            num_warmup_steps = 0, # Default value in run_glue.py\n","                                            num_training_steps = total_steps)"],"execution_count":14,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-14-951ba5298f86>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Total number of training steps is number of batches * number of epochs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtotal_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Create the learning rate scheduler.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'train_dataloader' is not defined"]}]},{"cell_type":"code","metadata":{"id":"Jx4qNMlUvaUT","executionInfo":{"status":"ok","timestamp":1621450249543,"user_tz":240,"elapsed":1088,"user":{"displayName":"Sydney Lister","photoUrl":"","userId":"17557786652639167110"}}},"source":["# Function to calculate the accuracy of our predictions vs labels\n","def flat_accuracy(preds, labels):\n","    pred_flat = np.argmax(preds, axis=1).flatten()\n","    labels_flat = labels.flatten()\n","    return np.sum(pred_flat == labels_flat) / len(labels_flat)"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"id":"5Z0ejqhPviJS","executionInfo":{"status":"ok","timestamp":1621450251116,"user_tz":240,"elapsed":896,"user":{"displayName":"Sydney Lister","photoUrl":"","userId":"17557786652639167110"}}},"source":["def format_time(elapsed):\n","    '''\n","    Takes a time in seconds and returns a string hh:mm:ss\n","    '''\n","    # Round to the nearest second.\n","    elapsed_rounded = int(round((elapsed)))\n","    \n","    # Format as hh:mm:ss\n","    return str(datetime.timedelta(seconds=elapsed_rounded))"],"execution_count":16,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"W4nx3LqmlU9w","executionInfo":{"status":"ok","timestamp":1621469724337,"user_tz":240,"elapsed":18638373,"user":{"displayName":"Sydney Lister","photoUrl":"","userId":"17557786652639167110"}},"outputId":"1f0c0151-2d66-4da7-807e-f2985796cab5"},"source":["from sklearn.model_selection import StratifiedKFold\n","from sklearn.linear_model import LogisticRegression\n","\n","\n","skf = StratifiedKFold(n_splits=10, random_state=2018, shuffle=True)\n","\n","tokenized = df.text.apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))\n","max_len = 0\n","for i in tokenized.values:\n","    if len(i) > max_len:\n","        max_len = len(i)\n","\n","padded = np.array([i + [0]*(max_len-len(i)) for i in tokenized.values])\n","attention_mask = np.where(padded != 0, 1, 0)\n","results = []\n","for train_index, test_index in skf.split(df.text, labels):\n","  X_train, X_val = padded[train_index], padded[test_index]\n","  train_mask, val_mask = attention_mask[train_index], attention_mask[test_index] \n","  y_train, y_val = labels[train_index], labels[test_index]\n","\n","\n","  # input_ids = torch.tensor(padded)  \n","  # attention_mask = torch.tensor(attention_mask)\n","\n","\n","  train_inputs = torch.tensor(X_train)\n","  validation_inputs = torch.tensor(X_val)\n","\n","  train_labels = torch.tensor(y_train)\n","  validation_labels = torch.tensor(y_val)\n","\n","  train_masks = torch.tensor(train_mask)\n","  validation_masks = torch.tensor(val_mask)\n","\n","\n","  batch_size = 16\n","\n","  # Create the DataLoader for our training set.\n","  train_data = TensorDataset(train_inputs, train_masks, train_labels)\n","  train_sampler = RandomSampler(train_data)\n","  train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n","\n","  # Create the DataLoader for our validation set.\n","  validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n","  validation_sampler = SequentialSampler(validation_data)\n","  validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n","  # Set the seed value all over the place to make this reproducible.\n","  seed_val = 42\n","\n","  random.seed(seed_val)\n","  np.random.seed(seed_val)\n","  torch.manual_seed(seed_val)\n","  torch.cuda.manual_seed_all(seed_val)\n","\n","  # Store the average loss after each epoch so we can plot them.\n","  loss_values = []\n","\n","\n","  # Number of training epochs (authors recommend between 2 and 4)\n","  epochs = 4\n","\n","  # Total number of training steps is number of batches * number of epochs.\n","  total_steps = len(train_dataloader) * epochs\n","\n","  # Create the learning rate scheduler.\n","  scheduler = get_linear_schedule_with_warmup(optimizer, \n","                                              num_warmup_steps = 0, # Default value in run_glue.py\n","                                              num_training_steps = total_steps)\n","\n","\n","  # For each epoch...\n","  for epoch_i in range(0, epochs):\n","      \n","      # ========================================\n","      #               Training\n","      # ========================================\n","      \n","      # Perform one full pass over the training set.\n","\n","      print(\"\")\n","      print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n","      print('Training...')\n","\n","      # Measure how long the training epoch takes.\n","      t0 = time.time()\n","\n","      # Reset the total loss for this epoch.\n","      total_loss = 0\n","\n","      # Put the model into training mode. Don't be mislead--the call to \n","      # `train` just changes the *mode*, it doesn't *perform* the training.\n","      # `dropout` and `batchnorm` layers behave differently during training\n","      # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n","      model.train()\n","\n","      # For each batch of training data...\n","      for step, batch in enumerate(train_dataloader):\n","\n","          # Progress update every 40 batches.\n","          if step % 40 == 0 and not step == 0:\n","              # Calculate elapsed time in minutes.\n","              elapsed = format_time(time.time() - t0)\n","              \n","              # Report progress.\n","              print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n","\n","          # Unpack this training batch from our dataloader. \n","          #\n","          # As we unpack the batch, we'll also copy each tensor to the GPU using the \n","          # `to` method.\n","          #\n","          # `batch` contains three pytorch tensors:\n","          #   [0]: input ids \n","          #   [1]: attention masks\n","          #   [2]: labels \n","          b_input_ids = batch[0].to(device)\n","          b_input_mask = batch[1].to(device)\n","          b_labels = batch[2].to(device)\n","          # b_labels.type(torch.LongTensor)\n","          # print(b_labels.type())\n","          b_labels = b_labels.type(torch.cuda.LongTensor)\n","          # b_labels = Variable(b_labels_temp, requires_grad=True).cuda()\n","\n","          # Always clear any previously calculated gradients before performing a\n","          # backward pass. PyTorch doesn't do this automatically because \n","          # accumulating the gradients is \"convenient while training RNNs\". \n","          # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n","          model.zero_grad()        \n","\n","          # Perform a forward pass (evaluate the model on this training batch).\n","          # This will return the loss (rather than the model output) because we\n","          # have provided the `labels`.\n","          # The documentation for this `model` function is here: \n","          # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n","          outputs = model(b_input_ids, \n","                      token_type_ids=None, \n","                      attention_mask=b_input_mask, \n","                      labels=b_labels)\n","          \n","          # The call to `model` always returns a tuple, so we need to pull the \n","          # loss value out of the tuple.\n","          loss = outputs[0]\n","\n","\n","          # features = outputs[0][:,0,:].numpy()\n","\n","          # train_features, test_features, train_labels, test_labels = train_test_split(features, b_labels)\n","\n","          # lr_clf = LogisticRegression()\n","          # lr_clf.fit(train_features, train_labels)\n","\n","          # Accumulate the training loss over all of the batches so that we can\n","          # calculate the average loss at the end. `loss` is a Tensor containing a\n","          # single value; the `.item()` function just returns the Python value \n","          # from the tensor.\n","          total_loss += loss.item()\n","\n","          # Perform a backward pass to calculate the gradients.\n","          loss.backward()\n","\n","          # Clip the norm of the gradients to 1.0.\n","          # This is to help prevent the \"exploding gradients\" problem.\n","          torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","\n","          # Update parameters and take a step using the computed gradient.\n","          # The optimizer dictates the \"update rule\"--how the parameters are\n","          # modified based on their gradients, the learning rate, etc.\n","          optimizer.step()\n","\n","          # Update the learning rate.\n","          scheduler.step()\n","\n","      # Calculate the average loss over the training data.\n","      avg_train_loss = total_loss / len(train_dataloader)            \n","      \n","      # Store the loss value for plotting the learning curve.\n","      loss_values.append(avg_train_loss)\n","\n","      print(\"\")\n","      print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n","      print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n","          \n","      # ========================================\n","      #               Validation\n","      # ========================================\n","      # After the completion of each training epoch, measure our performance on\n","      # our validation set.\n","\n","      print(\"\")\n","      print(\"Running Validation...\")\n","\n","      t0 = time.time()\n","\n","      # Put the model in evaluation mode--the dropout layers behave differently\n","      # during evaluation.\n","      model.eval()\n","\n","      # Tracking variables \n","      eval_loss, eval_accuracy = 0, 0\n","      nb_eval_steps, nb_eval_examples = 0, 0\n","\n","      # Evaluate data for one epoch\n","      for batch in validation_dataloader:\n","          \n","          # Add batch to GPU\n","          batch = tuple(t.to(device) for t in batch)\n","          \n","          # Unpack the inputs from our dataloader\n","          b_input_ids, b_input_mask, b_labels = batch\n","          \n","          # Telling the model not to compute or store gradients, saving memory and\n","          # speeding up validation\n","          with torch.no_grad():        \n","\n","              # Forward pass, calculate logit predictions.\n","              # This will return the logits rather than the loss because we have\n","              # not provided labels.\n","              # token_type_ids is the same as the \"segment ids\", which \n","              # differentiates sentence 1 and 2 in 2-sentence tasks.\n","              # The documentation for this `model` function is here: \n","              # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n","              outputs = model(b_input_ids, \n","                              token_type_ids=None, \n","                              attention_mask=b_input_mask)\n","          \n","          # Get the \"logits\" output by the model. The \"logits\" are the output\n","          # values prior to applying an activation function like the softmax.\n","          logits = outputs[0]\n","\n","          # # linear reg way \n","          # new_last_hidden_states = outputs[0].detach().numpy()[0][0]\n","          # lr_clf.predict_proba(new_last_hidden_states)\n","\n","          # Move logits and labels to CPU\n","          logits = logits.detach().cpu().numpy()\n","          label_ids = b_labels.to('cpu').numpy()\n","          \n","          # Calculate the accuracy for this batch of test sentences.\n","          tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n","          \n","          # Accumulate the total accuracy.\n","          eval_accuracy += tmp_eval_accuracy\n","\n","          # Track the number of batches\n","          nb_eval_steps += 1\n","\n","      # Report the final accuracy for this validation run.\n","      print(\"  Accuracy: \" + str(eval_accuracy/nb_eval_steps))\n","      print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n","\n","  print(\"\")\n","  print(\"Training complete!\")\n","  results.extend(loss_values)\n","print(sum(results)/len(results))\n"],"execution_count":23,"outputs":[{"output_type":"stream","text":["\n","======== Epoch 1 / 4 ========\n","Training...\n","  Batch    40  of    837.    Elapsed: 0:00:22.\n","  Batch    80  of    837.    Elapsed: 0:00:43.\n","  Batch   120  of    837.    Elapsed: 0:01:05.\n","  Batch   160  of    837.    Elapsed: 0:01:26.\n","  Batch   200  of    837.    Elapsed: 0:01:47.\n","  Batch   240  of    837.    Elapsed: 0:02:09.\n","  Batch   280  of    837.    Elapsed: 0:02:30.\n","  Batch   320  of    837.    Elapsed: 0:02:52.\n","  Batch   360  of    837.    Elapsed: 0:03:13.\n","  Batch   400  of    837.    Elapsed: 0:03:35.\n","  Batch   440  of    837.    Elapsed: 0:03:56.\n","  Batch   480  of    837.    Elapsed: 0:04:18.\n","  Batch   520  of    837.    Elapsed: 0:04:39.\n","  Batch   560  of    837.    Elapsed: 0:05:01.\n","  Batch   600  of    837.    Elapsed: 0:05:22.\n","  Batch   640  of    837.    Elapsed: 0:05:44.\n","  Batch   680  of    837.    Elapsed: 0:06:05.\n","  Batch   720  of    837.    Elapsed: 0:06:27.\n","  Batch   760  of    837.    Elapsed: 0:06:48.\n","  Batch   800  of    837.    Elapsed: 0:07:10.\n","\n","  Average training loss: 0.00\n","  Training epcoh took: 0:07:29\n","\n","Running Validation...\n","  Accuracy: 0.9986559139784946\n","  Validation took: 0:00:17\n","\n","======== Epoch 2 / 4 ========\n","Training...\n","  Batch    40  of    837.    Elapsed: 0:00:21.\n","  Batch    80  of    837.    Elapsed: 0:00:43.\n","  Batch   120  of    837.    Elapsed: 0:01:04.\n","  Batch   160  of    837.    Elapsed: 0:01:26.\n","  Batch   200  of    837.    Elapsed: 0:01:47.\n","  Batch   240  of    837.    Elapsed: 0:02:09.\n","  Batch   280  of    837.    Elapsed: 0:02:30.\n","  Batch   320  of    837.    Elapsed: 0:02:52.\n","  Batch   360  of    837.    Elapsed: 0:03:13.\n","  Batch   400  of    837.    Elapsed: 0:03:35.\n","  Batch   440  of    837.    Elapsed: 0:03:56.\n","  Batch   480  of    837.    Elapsed: 0:04:18.\n","  Batch   520  of    837.    Elapsed: 0:04:39.\n","  Batch   560  of    837.    Elapsed: 0:05:01.\n","  Batch   600  of    837.    Elapsed: 0:05:22.\n","  Batch   640  of    837.    Elapsed: 0:05:44.\n","  Batch   680  of    837.    Elapsed: 0:06:05.\n","  Batch   720  of    837.    Elapsed: 0:06:27.\n","  Batch   760  of    837.    Elapsed: 0:06:48.\n","  Batch   800  of    837.    Elapsed: 0:07:10.\n","\n","  Average training loss: 0.00\n","  Training epcoh took: 0:07:29\n","\n","Running Validation...\n","  Accuracy: 1.0\n","  Validation took: 0:00:17\n","\n","======== Epoch 3 / 4 ========\n","Training...\n","  Batch    40  of    837.    Elapsed: 0:00:21.\n","  Batch    80  of    837.    Elapsed: 0:00:43.\n","  Batch   120  of    837.    Elapsed: 0:01:04.\n","  Batch   160  of    837.    Elapsed: 0:01:26.\n","  Batch   200  of    837.    Elapsed: 0:01:47.\n","  Batch   240  of    837.    Elapsed: 0:02:09.\n","  Batch   280  of    837.    Elapsed: 0:02:30.\n","  Batch   320  of    837.    Elapsed: 0:02:52.\n","  Batch   360  of    837.    Elapsed: 0:03:13.\n","  Batch   400  of    837.    Elapsed: 0:03:35.\n","  Batch   440  of    837.    Elapsed: 0:03:56.\n","  Batch   480  of    837.    Elapsed: 0:04:18.\n","  Batch   520  of    837.    Elapsed: 0:04:39.\n","  Batch   560  of    837.    Elapsed: 0:05:01.\n","  Batch   600  of    837.    Elapsed: 0:05:22.\n","  Batch   640  of    837.    Elapsed: 0:05:44.\n","  Batch   680  of    837.    Elapsed: 0:06:05.\n","  Batch   720  of    837.    Elapsed: 0:06:27.\n","  Batch   760  of    837.    Elapsed: 0:06:48.\n","  Batch   800  of    837.    Elapsed: 0:07:10.\n","\n","  Average training loss: 0.00\n","  Training epcoh took: 0:07:29\n","\n","Running Validation...\n","  Accuracy: 1.0\n","  Validation took: 0:00:17\n","\n","======== Epoch 4 / 4 ========\n","Training...\n","  Batch    40  of    837.    Elapsed: 0:00:21.\n","  Batch    80  of    837.    Elapsed: 0:00:43.\n","  Batch   120  of    837.    Elapsed: 0:01:04.\n","  Batch   160  of    837.    Elapsed: 0:01:26.\n","  Batch   200  of    837.    Elapsed: 0:01:47.\n","  Batch   240  of    837.    Elapsed: 0:02:09.\n","  Batch   280  of    837.    Elapsed: 0:02:30.\n","  Batch   320  of    837.    Elapsed: 0:02:52.\n","  Batch   360  of    837.    Elapsed: 0:03:13.\n","  Batch   400  of    837.    Elapsed: 0:03:35.\n","  Batch   440  of    837.    Elapsed: 0:03:56.\n","  Batch   480  of    837.    Elapsed: 0:04:18.\n","  Batch   520  of    837.    Elapsed: 0:04:39.\n","  Batch   560  of    837.    Elapsed: 0:05:01.\n","  Batch   600  of    837.    Elapsed: 0:05:22.\n","  Batch   640  of    837.    Elapsed: 0:05:44.\n","  Batch   680  of    837.    Elapsed: 0:06:05.\n","  Batch   720  of    837.    Elapsed: 0:06:27.\n","  Batch   760  of    837.    Elapsed: 0:06:48.\n","  Batch   800  of    837.    Elapsed: 0:07:10.\n","\n","  Average training loss: 0.00\n","  Training epcoh took: 0:07:29\n","\n","Running Validation...\n","  Accuracy: 0.9993279569892473\n","  Validation took: 0:00:17\n","\n","Training complete!\n","\n","======== Epoch 1 / 4 ========\n","Training...\n","  Batch    40  of    837.    Elapsed: 0:00:21.\n","  Batch    80  of    837.    Elapsed: 0:00:43.\n","  Batch   120  of    837.    Elapsed: 0:01:04.\n","  Batch   160  of    837.    Elapsed: 0:01:26.\n","  Batch   200  of    837.    Elapsed: 0:01:47.\n","  Batch   240  of    837.    Elapsed: 0:02:09.\n","  Batch   280  of    837.    Elapsed: 0:02:30.\n","  Batch   320  of    837.    Elapsed: 0:02:52.\n","  Batch   360  of    837.    Elapsed: 0:03:13.\n","  Batch   400  of    837.    Elapsed: 0:03:35.\n","  Batch   440  of    837.    Elapsed: 0:03:56.\n","  Batch   480  of    837.    Elapsed: 0:04:18.\n","  Batch   520  of    837.    Elapsed: 0:04:39.\n","  Batch   560  of    837.    Elapsed: 0:05:01.\n","  Batch   600  of    837.    Elapsed: 0:05:22.\n","  Batch   640  of    837.    Elapsed: 0:05:44.\n","  Batch   680  of    837.    Elapsed: 0:06:05.\n","  Batch   720  of    837.    Elapsed: 0:06:27.\n","  Batch   760  of    837.    Elapsed: 0:06:48.\n","  Batch   800  of    837.    Elapsed: 0:07:10.\n","\n","  Average training loss: 0.00\n","  Training epcoh took: 0:07:29\n","\n","Running Validation...\n","  Accuracy: 1.0\n","  Validation took: 0:00:17\n","\n","======== Epoch 2 / 4 ========\n","Training...\n","  Batch    40  of    837.    Elapsed: 0:00:21.\n","  Batch    80  of    837.    Elapsed: 0:00:43.\n","  Batch   120  of    837.    Elapsed: 0:01:05.\n","  Batch   160  of    837.    Elapsed: 0:01:26.\n","  Batch   200  of    837.    Elapsed: 0:01:48.\n","  Batch   240  of    837.    Elapsed: 0:02:09.\n","  Batch   280  of    837.    Elapsed: 0:02:30.\n","  Batch   320  of    837.    Elapsed: 0:02:52.\n","  Batch   360  of    837.    Elapsed: 0:03:13.\n","  Batch   400  of    837.    Elapsed: 0:03:35.\n","  Batch   440  of    837.    Elapsed: 0:03:56.\n","  Batch   480  of    837.    Elapsed: 0:04:18.\n","  Batch   520  of    837.    Elapsed: 0:04:39.\n","  Batch   560  of    837.    Elapsed: 0:05:01.\n","  Batch   600  of    837.    Elapsed: 0:05:22.\n","  Batch   640  of    837.    Elapsed: 0:05:44.\n","  Batch   680  of    837.    Elapsed: 0:06:05.\n","  Batch   720  of    837.    Elapsed: 0:06:27.\n","  Batch   760  of    837.    Elapsed: 0:06:48.\n","  Batch   800  of    837.    Elapsed: 0:07:10.\n","\n","  Average training loss: 0.00\n","  Training epcoh took: 0:07:29\n","\n","Running Validation...\n","  Accuracy: 1.0\n","  Validation took: 0:00:17\n","\n","======== Epoch 3 / 4 ========\n","Training...\n","  Batch    40  of    837.    Elapsed: 0:00:21.\n","  Batch    80  of    837.    Elapsed: 0:00:43.\n","  Batch   120  of    837.    Elapsed: 0:01:04.\n","  Batch   160  of    837.    Elapsed: 0:01:26.\n","  Batch   200  of    837.    Elapsed: 0:01:47.\n","  Batch   240  of    837.    Elapsed: 0:02:09.\n","  Batch   280  of    837.    Elapsed: 0:02:30.\n","  Batch   320  of    837.    Elapsed: 0:02:52.\n","  Batch   360  of    837.    Elapsed: 0:03:13.\n","  Batch   400  of    837.    Elapsed: 0:03:35.\n","  Batch   440  of    837.    Elapsed: 0:03:56.\n","  Batch   480  of    837.    Elapsed: 0:04:18.\n","  Batch   520  of    837.    Elapsed: 0:04:39.\n","  Batch   560  of    837.    Elapsed: 0:05:01.\n","  Batch   600  of    837.    Elapsed: 0:05:22.\n","  Batch   640  of    837.    Elapsed: 0:05:44.\n","  Batch   680  of    837.    Elapsed: 0:06:05.\n","  Batch   720  of    837.    Elapsed: 0:06:27.\n","  Batch   760  of    837.    Elapsed: 0:06:48.\n","  Batch   800  of    837.    Elapsed: 0:07:10.\n","\n","  Average training loss: 0.00\n","  Training epcoh took: 0:07:29\n","\n","Running Validation...\n","  Accuracy: 1.0\n","  Validation took: 0:00:17\n","\n","======== Epoch 4 / 4 ========\n","Training...\n","  Batch    40  of    837.    Elapsed: 0:00:21.\n","  Batch    80  of    837.    Elapsed: 0:00:43.\n","  Batch   120  of    837.    Elapsed: 0:01:04.\n","  Batch   160  of    837.    Elapsed: 0:01:26.\n","  Batch   200  of    837.    Elapsed: 0:01:47.\n","  Batch   240  of    837.    Elapsed: 0:02:09.\n","  Batch   280  of    837.    Elapsed: 0:02:30.\n","  Batch   320  of    837.    Elapsed: 0:02:52.\n","  Batch   360  of    837.    Elapsed: 0:03:13.\n","  Batch   400  of    837.    Elapsed: 0:03:35.\n","  Batch   440  of    837.    Elapsed: 0:03:56.\n","  Batch   480  of    837.    Elapsed: 0:04:18.\n","  Batch   520  of    837.    Elapsed: 0:04:39.\n","  Batch   560  of    837.    Elapsed: 0:05:01.\n","  Batch   600  of    837.    Elapsed: 0:05:22.\n","  Batch   640  of    837.    Elapsed: 0:05:44.\n","  Batch   680  of    837.    Elapsed: 0:06:05.\n","  Batch   720  of    837.    Elapsed: 0:06:27.\n","  Batch   760  of    837.    Elapsed: 0:06:48.\n","  Batch   800  of    837.    Elapsed: 0:07:10.\n","\n","  Average training loss: 0.00\n","  Training epcoh took: 0:07:29\n","\n","Running Validation...\n","  Accuracy: 1.0\n","  Validation took: 0:00:17\n","\n","Training complete!\n","\n","======== Epoch 1 / 4 ========\n","Training...\n","  Batch    40  of    837.    Elapsed: 0:00:21.\n","  Batch    80  of    837.    Elapsed: 0:00:43.\n","  Batch   120  of    837.    Elapsed: 0:01:04.\n","  Batch   160  of    837.    Elapsed: 0:01:26.\n","  Batch   200  of    837.    Elapsed: 0:01:47.\n","  Batch   240  of    837.    Elapsed: 0:02:09.\n","  Batch   280  of    837.    Elapsed: 0:02:30.\n","  Batch   320  of    837.    Elapsed: 0:02:52.\n","  Batch   360  of    837.    Elapsed: 0:03:13.\n","  Batch   400  of    837.    Elapsed: 0:03:35.\n","  Batch   440  of    837.    Elapsed: 0:03:56.\n","  Batch   480  of    837.    Elapsed: 0:04:18.\n","  Batch   520  of    837.    Elapsed: 0:04:39.\n","  Batch   560  of    837.    Elapsed: 0:05:01.\n","  Batch   600  of    837.    Elapsed: 0:05:22.\n","  Batch   640  of    837.    Elapsed: 0:05:44.\n","  Batch   680  of    837.    Elapsed: 0:06:05.\n","  Batch   720  of    837.    Elapsed: 0:06:27.\n","  Batch   760  of    837.    Elapsed: 0:06:48.\n","  Batch   800  of    837.    Elapsed: 0:07:10.\n","\n","  Average training loss: 0.00\n","  Training epcoh took: 0:07:29\n","\n","Running Validation...\n","  Accuracy: 1.0\n","  Validation took: 0:00:17\n","\n","======== Epoch 2 / 4 ========\n","Training...\n","  Batch    40  of    837.    Elapsed: 0:00:21.\n","  Batch    80  of    837.    Elapsed: 0:00:43.\n","  Batch   120  of    837.    Elapsed: 0:01:04.\n","  Batch   160  of    837.    Elapsed: 0:01:26.\n","  Batch   200  of    837.    Elapsed: 0:01:47.\n","  Batch   240  of    837.    Elapsed: 0:02:09.\n","  Batch   280  of    837.    Elapsed: 0:02:30.\n","  Batch   320  of    837.    Elapsed: 0:02:52.\n","  Batch   360  of    837.    Elapsed: 0:03:13.\n","  Batch   400  of    837.    Elapsed: 0:03:35.\n","  Batch   440  of    837.    Elapsed: 0:03:56.\n","  Batch   480  of    837.    Elapsed: 0:04:18.\n","  Batch   520  of    837.    Elapsed: 0:04:39.\n","  Batch   560  of    837.    Elapsed: 0:05:01.\n","  Batch   600  of    837.    Elapsed: 0:05:22.\n","  Batch   640  of    837.    Elapsed: 0:05:44.\n","  Batch   680  of    837.    Elapsed: 0:06:05.\n","  Batch   720  of    837.    Elapsed: 0:06:27.\n","  Batch   760  of    837.    Elapsed: 0:06:48.\n","  Batch   800  of    837.    Elapsed: 0:07:10.\n","\n","  Average training loss: 0.00\n","  Training epcoh took: 0:07:29\n","\n","Running Validation...\n","  Accuracy: 1.0\n","  Validation took: 0:00:17\n","\n","======== Epoch 3 / 4 ========\n","Training...\n","  Batch    40  of    837.    Elapsed: 0:00:21.\n","  Batch    80  of    837.    Elapsed: 0:00:43.\n","  Batch   120  of    837.    Elapsed: 0:01:04.\n","  Batch   160  of    837.    Elapsed: 0:01:26.\n","  Batch   200  of    837.    Elapsed: 0:01:47.\n","  Batch   240  of    837.    Elapsed: 0:02:09.\n","  Batch   280  of    837.    Elapsed: 0:02:30.\n","  Batch   320  of    837.    Elapsed: 0:02:52.\n","  Batch   360  of    837.    Elapsed: 0:03:13.\n","  Batch   400  of    837.    Elapsed: 0:03:35.\n","  Batch   440  of    837.    Elapsed: 0:03:56.\n","  Batch   480  of    837.    Elapsed: 0:04:18.\n","  Batch   520  of    837.    Elapsed: 0:04:39.\n","  Batch   560  of    837.    Elapsed: 0:05:01.\n","  Batch   600  of    837.    Elapsed: 0:05:22.\n","  Batch   640  of    837.    Elapsed: 0:05:44.\n","  Batch   680  of    837.    Elapsed: 0:06:05.\n","  Batch   720  of    837.    Elapsed: 0:06:27.\n","  Batch   760  of    837.    Elapsed: 0:06:48.\n","  Batch   800  of    837.    Elapsed: 0:07:10.\n","\n","  Average training loss: 0.00\n","  Training epcoh took: 0:07:29\n","\n","Running Validation...\n","  Accuracy: 1.0\n","  Validation took: 0:00:17\n","\n","======== Epoch 4 / 4 ========\n","Training...\n","  Batch    40  of    837.    Elapsed: 0:00:21.\n","  Batch    80  of    837.    Elapsed: 0:00:43.\n","  Batch   120  of    837.    Elapsed: 0:01:04.\n","  Batch   160  of    837.    Elapsed: 0:01:26.\n","  Batch   200  of    837.    Elapsed: 0:01:47.\n","  Batch   240  of    837.    Elapsed: 0:02:09.\n","  Batch   280  of    837.    Elapsed: 0:02:30.\n","  Batch   320  of    837.    Elapsed: 0:02:52.\n","  Batch   360  of    837.    Elapsed: 0:03:13.\n","  Batch   400  of    837.    Elapsed: 0:03:35.\n","  Batch   440  of    837.    Elapsed: 0:03:56.\n","  Batch   480  of    837.    Elapsed: 0:04:18.\n","  Batch   520  of    837.    Elapsed: 0:04:39.\n","  Batch   560  of    837.    Elapsed: 0:05:01.\n","  Batch   600  of    837.    Elapsed: 0:05:22.\n","  Batch   640  of    837.    Elapsed: 0:05:44.\n","  Batch   680  of    837.    Elapsed: 0:06:05.\n","  Batch   720  of    837.    Elapsed: 0:06:27.\n","  Batch   760  of    837.    Elapsed: 0:06:48.\n","  Batch   800  of    837.    Elapsed: 0:07:10.\n","\n","  Average training loss: 0.00\n","  Training epcoh took: 0:07:29\n","\n","Running Validation...\n","  Accuracy: 1.0\n","  Validation took: 0:00:17\n","\n","Training complete!\n","\n","======== Epoch 1 / 4 ========\n","Training...\n","  Batch    40  of    837.    Elapsed: 0:00:21.\n","  Batch    80  of    837.    Elapsed: 0:00:43.\n","  Batch   120  of    837.    Elapsed: 0:01:04.\n","  Batch   160  of    837.    Elapsed: 0:01:26.\n","  Batch   200  of    837.    Elapsed: 0:01:47.\n","  Batch   240  of    837.    Elapsed: 0:02:09.\n","  Batch   280  of    837.    Elapsed: 0:02:30.\n","  Batch   320  of    837.    Elapsed: 0:02:52.\n","  Batch   360  of    837.    Elapsed: 0:03:14.\n","  Batch   400  of    837.    Elapsed: 0:03:35.\n","  Batch   440  of    837.    Elapsed: 0:03:57.\n","  Batch   480  of    837.    Elapsed: 0:04:18.\n","  Batch   520  of    837.    Elapsed: 0:04:40.\n","  Batch   560  of    837.    Elapsed: 0:05:01.\n","  Batch   600  of    837.    Elapsed: 0:05:23.\n","  Batch   640  of    837.    Elapsed: 0:05:44.\n","  Batch   680  of    837.    Elapsed: 0:06:06.\n","  Batch   720  of    837.    Elapsed: 0:06:27.\n","  Batch   760  of    837.    Elapsed: 0:06:49.\n","  Batch   800  of    837.    Elapsed: 0:07:10.\n","\n","  Average training loss: 0.00\n","  Training epcoh took: 0:07:30\n","\n","Running Validation...\n","  Accuracy: 1.0\n","  Validation took: 0:00:17\n","\n","======== Epoch 2 / 4 ========\n","Training...\n","  Batch    40  of    837.    Elapsed: 0:00:21.\n","  Batch    80  of    837.    Elapsed: 0:00:43.\n","  Batch   120  of    837.    Elapsed: 0:01:04.\n","  Batch   160  of    837.    Elapsed: 0:01:26.\n","  Batch   200  of    837.    Elapsed: 0:01:48.\n","  Batch   240  of    837.    Elapsed: 0:02:09.\n","  Batch   280  of    837.    Elapsed: 0:02:31.\n","  Batch   320  of    837.    Elapsed: 0:02:52.\n","  Batch   360  of    837.    Elapsed: 0:03:14.\n","  Batch   400  of    837.    Elapsed: 0:03:35.\n","  Batch   440  of    837.    Elapsed: 0:03:57.\n","  Batch   480  of    837.    Elapsed: 0:04:18.\n","  Batch   520  of    837.    Elapsed: 0:04:40.\n","  Batch   560  of    837.    Elapsed: 0:05:01.\n","  Batch   600  of    837.    Elapsed: 0:05:23.\n","  Batch   640  of    837.    Elapsed: 0:05:44.\n","  Batch   680  of    837.    Elapsed: 0:06:05.\n","  Batch   720  of    837.    Elapsed: 0:06:27.\n","  Batch   760  of    837.    Elapsed: 0:06:48.\n","  Batch   800  of    837.    Elapsed: 0:07:10.\n","\n","  Average training loss: 0.00\n","  Training epcoh took: 0:07:29\n","\n","Running Validation...\n","  Accuracy: 1.0\n","  Validation took: 0:00:17\n","\n","======== Epoch 3 / 4 ========\n","Training...\n","  Batch    40  of    837.    Elapsed: 0:00:22.\n","  Batch    80  of    837.    Elapsed: 0:00:43.\n","  Batch   120  of    837.    Elapsed: 0:01:04.\n","  Batch   160  of    837.    Elapsed: 0:01:26.\n","  Batch   200  of    837.    Elapsed: 0:01:47.\n","  Batch   240  of    837.    Elapsed: 0:02:09.\n","  Batch   280  of    837.    Elapsed: 0:02:30.\n","  Batch   320  of    837.    Elapsed: 0:02:52.\n","  Batch   360  of    837.    Elapsed: 0:03:13.\n","  Batch   400  of    837.    Elapsed: 0:03:35.\n","  Batch   440  of    837.    Elapsed: 0:03:56.\n","  Batch   480  of    837.    Elapsed: 0:04:18.\n","  Batch   520  of    837.    Elapsed: 0:04:39.\n","  Batch   560  of    837.    Elapsed: 0:05:01.\n","  Batch   600  of    837.    Elapsed: 0:05:22.\n","  Batch   640  of    837.    Elapsed: 0:05:44.\n","  Batch   680  of    837.    Elapsed: 0:06:05.\n","  Batch   720  of    837.    Elapsed: 0:06:27.\n","  Batch   760  of    837.    Elapsed: 0:06:48.\n","  Batch   800  of    837.    Elapsed: 0:07:10.\n","\n","  Average training loss: 0.00\n","  Training epcoh took: 0:07:29\n","\n","Running Validation...\n","  Accuracy: 1.0\n","  Validation took: 0:00:17\n","\n","======== Epoch 4 / 4 ========\n","Training...\n","  Batch    40  of    837.    Elapsed: 0:00:22.\n","  Batch    80  of    837.    Elapsed: 0:00:43.\n","  Batch   120  of    837.    Elapsed: 0:01:04.\n","  Batch   160  of    837.    Elapsed: 0:01:26.\n","  Batch   200  of    837.    Elapsed: 0:01:47.\n","  Batch   240  of    837.    Elapsed: 0:02:09.\n","  Batch   280  of    837.    Elapsed: 0:02:30.\n","  Batch   320  of    837.    Elapsed: 0:02:52.\n","  Batch   360  of    837.    Elapsed: 0:03:13.\n","  Batch   400  of    837.    Elapsed: 0:03:35.\n","  Batch   440  of    837.    Elapsed: 0:03:56.\n","  Batch   480  of    837.    Elapsed: 0:04:18.\n","  Batch   520  of    837.    Elapsed: 0:04:39.\n","  Batch   560  of    837.    Elapsed: 0:05:01.\n","  Batch   600  of    837.    Elapsed: 0:05:22.\n","  Batch   640  of    837.    Elapsed: 0:05:44.\n","  Batch   680  of    837.    Elapsed: 0:06:05.\n","  Batch   720  of    837.    Elapsed: 0:06:27.\n","  Batch   760  of    837.    Elapsed: 0:06:48.\n","  Batch   800  of    837.    Elapsed: 0:07:10.\n","\n","  Average training loss: 0.00\n","  Training epcoh took: 0:07:29\n","\n","Running Validation...\n","  Accuracy: 1.0\n","  Validation took: 0:00:17\n","\n","Training complete!\n","\n","======== Epoch 1 / 4 ========\n","Training...\n","  Batch    40  of    837.    Elapsed: 0:00:21.\n","  Batch    80  of    837.    Elapsed: 0:00:43.\n","  Batch   120  of    837.    Elapsed: 0:01:04.\n","  Batch   160  of    837.    Elapsed: 0:01:26.\n","  Batch   200  of    837.    Elapsed: 0:01:47.\n","  Batch   240  of    837.    Elapsed: 0:02:09.\n","  Batch   280  of    837.    Elapsed: 0:02:30.\n","  Batch   320  of    837.    Elapsed: 0:02:52.\n","  Batch   360  of    837.    Elapsed: 0:03:13.\n","  Batch   400  of    837.    Elapsed: 0:03:35.\n","  Batch   440  of    837.    Elapsed: 0:03:56.\n","  Batch   480  of    837.    Elapsed: 0:04:18.\n","  Batch   520  of    837.    Elapsed: 0:04:39.\n","  Batch   560  of    837.    Elapsed: 0:05:01.\n","  Batch   600  of    837.    Elapsed: 0:05:22.\n","  Batch   640  of    837.    Elapsed: 0:05:44.\n","  Batch   680  of    837.    Elapsed: 0:06:05.\n","  Batch   720  of    837.    Elapsed: 0:06:27.\n","  Batch   760  of    837.    Elapsed: 0:06:48.\n","  Batch   800  of    837.    Elapsed: 0:07:10.\n","\n","  Average training loss: 0.00\n","  Training epcoh took: 0:07:29\n","\n","Running Validation...\n","  Accuracy: 1.0\n","  Validation took: 0:00:17\n","\n","======== Epoch 2 / 4 ========\n","Training...\n","  Batch    40  of    837.    Elapsed: 0:00:21.\n","  Batch    80  of    837.    Elapsed: 0:00:43.\n","  Batch   120  of    837.    Elapsed: 0:01:04.\n","  Batch   160  of    837.    Elapsed: 0:01:26.\n","  Batch   200  of    837.    Elapsed: 0:01:47.\n","  Batch   240  of    837.    Elapsed: 0:02:09.\n","  Batch   280  of    837.    Elapsed: 0:02:30.\n","  Batch   320  of    837.    Elapsed: 0:02:52.\n","  Batch   360  of    837.    Elapsed: 0:03:13.\n","  Batch   400  of    837.    Elapsed: 0:03:35.\n","  Batch   440  of    837.    Elapsed: 0:03:56.\n","  Batch   480  of    837.    Elapsed: 0:04:18.\n","  Batch   520  of    837.    Elapsed: 0:04:39.\n","  Batch   560  of    837.    Elapsed: 0:05:01.\n","  Batch   600  of    837.    Elapsed: 0:05:22.\n","  Batch   640  of    837.    Elapsed: 0:05:44.\n","  Batch   680  of    837.    Elapsed: 0:06:05.\n","  Batch   720  of    837.    Elapsed: 0:06:26.\n","  Batch   760  of    837.    Elapsed: 0:06:48.\n","  Batch   800  of    837.    Elapsed: 0:07:09.\n","\n","  Average training loss: 0.00\n","  Training epcoh took: 0:07:29\n","\n","Running Validation...\n","  Accuracy: 1.0\n","  Validation took: 0:00:17\n","\n","======== Epoch 3 / 4 ========\n","Training...\n","  Batch    40  of    837.    Elapsed: 0:00:21.\n","  Batch    80  of    837.    Elapsed: 0:00:43.\n","  Batch   120  of    837.    Elapsed: 0:01:04.\n","  Batch   160  of    837.    Elapsed: 0:01:26.\n","  Batch   200  of    837.    Elapsed: 0:01:47.\n","  Batch   240  of    837.    Elapsed: 0:02:09.\n","  Batch   280  of    837.    Elapsed: 0:02:30.\n","  Batch   320  of    837.    Elapsed: 0:02:52.\n","  Batch   360  of    837.    Elapsed: 0:03:13.\n","  Batch   400  of    837.    Elapsed: 0:03:35.\n","  Batch   440  of    837.    Elapsed: 0:03:56.\n","  Batch   480  of    837.    Elapsed: 0:04:18.\n","  Batch   520  of    837.    Elapsed: 0:04:39.\n","  Batch   560  of    837.    Elapsed: 0:05:01.\n","  Batch   600  of    837.    Elapsed: 0:05:22.\n","  Batch   640  of    837.    Elapsed: 0:05:43.\n","  Batch   680  of    837.    Elapsed: 0:06:05.\n","  Batch   720  of    837.    Elapsed: 0:06:26.\n","  Batch   760  of    837.    Elapsed: 0:06:48.\n","  Batch   800  of    837.    Elapsed: 0:07:09.\n","\n","  Average training loss: 0.00\n","  Training epcoh took: 0:07:29\n","\n","Running Validation...\n","  Accuracy: 1.0\n","  Validation took: 0:00:17\n","\n","======== Epoch 4 / 4 ========\n","Training...\n","  Batch    40  of    837.    Elapsed: 0:00:21.\n","  Batch    80  of    837.    Elapsed: 0:00:43.\n","  Batch   120  of    837.    Elapsed: 0:01:04.\n","  Batch   160  of    837.    Elapsed: 0:01:26.\n","  Batch   200  of    837.    Elapsed: 0:01:47.\n","  Batch   240  of    837.    Elapsed: 0:02:09.\n","  Batch   280  of    837.    Elapsed: 0:02:30.\n","  Batch   320  of    837.    Elapsed: 0:02:52.\n","  Batch   360  of    837.    Elapsed: 0:03:13.\n","  Batch   400  of    837.    Elapsed: 0:03:35.\n","  Batch   440  of    837.    Elapsed: 0:03:56.\n","  Batch   480  of    837.    Elapsed: 0:04:18.\n","  Batch   520  of    837.    Elapsed: 0:04:39.\n","  Batch   560  of    837.    Elapsed: 0:05:01.\n","  Batch   600  of    837.    Elapsed: 0:05:22.\n","  Batch   640  of    837.    Elapsed: 0:05:44.\n","  Batch   680  of    837.    Elapsed: 0:06:05.\n","  Batch   720  of    837.    Elapsed: 0:06:26.\n","  Batch   760  of    837.    Elapsed: 0:06:48.\n","  Batch   800  of    837.    Elapsed: 0:07:09.\n","\n","  Average training loss: 0.00\n","  Training epcoh took: 0:07:29\n","\n","Running Validation...\n","  Accuracy: 1.0\n","  Validation took: 0:00:17\n","\n","Training complete!\n","\n","======== Epoch 1 / 4 ========\n","Training...\n","  Batch    40  of    837.    Elapsed: 0:00:21.\n","  Batch    80  of    837.    Elapsed: 0:00:43.\n","  Batch   120  of    837.    Elapsed: 0:01:04.\n","  Batch   160  of    837.    Elapsed: 0:01:26.\n","  Batch   200  of    837.    Elapsed: 0:01:47.\n","  Batch   240  of    837.    Elapsed: 0:02:09.\n","  Batch   280  of    837.    Elapsed: 0:02:30.\n","  Batch   320  of    837.    Elapsed: 0:02:52.\n","  Batch   360  of    837.    Elapsed: 0:03:13.\n","  Batch   400  of    837.    Elapsed: 0:03:35.\n","  Batch   440  of    837.    Elapsed: 0:03:56.\n","  Batch   480  of    837.    Elapsed: 0:04:18.\n","  Batch   520  of    837.    Elapsed: 0:04:39.\n","  Batch   560  of    837.    Elapsed: 0:05:01.\n","  Batch   600  of    837.    Elapsed: 0:05:22.\n","  Batch   640  of    837.    Elapsed: 0:05:44.\n","  Batch   680  of    837.    Elapsed: 0:06:05.\n","  Batch   720  of    837.    Elapsed: 0:06:27.\n","  Batch   760  of    837.    Elapsed: 0:06:48.\n","  Batch   800  of    837.    Elapsed: 0:07:10.\n","\n","  Average training loss: 0.00\n","  Training epcoh took: 0:07:29\n","\n","Running Validation...\n","  Accuracy: 1.0\n","  Validation took: 0:00:17\n","\n","======== Epoch 2 / 4 ========\n","Training...\n","  Batch    40  of    837.    Elapsed: 0:00:21.\n","  Batch    80  of    837.    Elapsed: 0:00:43.\n","  Batch   120  of    837.    Elapsed: 0:01:04.\n","  Batch   160  of    837.    Elapsed: 0:01:26.\n","  Batch   200  of    837.    Elapsed: 0:01:48.\n","  Batch   240  of    837.    Elapsed: 0:02:09.\n","  Batch   280  of    837.    Elapsed: 0:02:31.\n","  Batch   320  of    837.    Elapsed: 0:02:52.\n","  Batch   360  of    837.    Elapsed: 0:03:14.\n","  Batch   400  of    837.    Elapsed: 0:03:35.\n","  Batch   440  of    837.    Elapsed: 0:03:56.\n","  Batch   480  of    837.    Elapsed: 0:04:18.\n","  Batch   520  of    837.    Elapsed: 0:04:39.\n","  Batch   560  of    837.    Elapsed: 0:05:01.\n","  Batch   600  of    837.    Elapsed: 0:05:22.\n","  Batch   640  of    837.    Elapsed: 0:05:44.\n","  Batch   680  of    837.    Elapsed: 0:06:05.\n","  Batch   720  of    837.    Elapsed: 0:06:27.\n","  Batch   760  of    837.    Elapsed: 0:06:49.\n","  Batch   800  of    837.    Elapsed: 0:07:10.\n","\n","  Average training loss: 0.00\n","  Training epcoh took: 0:07:30\n","\n","Running Validation...\n","  Accuracy: 1.0\n","  Validation took: 0:00:17\n","\n","======== Epoch 3 / 4 ========\n","Training...\n","  Batch    40  of    837.    Elapsed: 0:00:22.\n","  Batch    80  of    837.    Elapsed: 0:00:43.\n","  Batch   120  of    837.    Elapsed: 0:01:04.\n","  Batch   160  of    837.    Elapsed: 0:01:26.\n","  Batch   200  of    837.    Elapsed: 0:01:47.\n","  Batch   240  of    837.    Elapsed: 0:02:09.\n","  Batch   280  of    837.    Elapsed: 0:02:30.\n","  Batch   320  of    837.    Elapsed: 0:02:52.\n","  Batch   360  of    837.    Elapsed: 0:03:14.\n","  Batch   400  of    837.    Elapsed: 0:03:35.\n","  Batch   440  of    837.    Elapsed: 0:03:56.\n","  Batch   480  of    837.    Elapsed: 0:04:18.\n","  Batch   520  of    837.    Elapsed: 0:04:39.\n","  Batch   560  of    837.    Elapsed: 0:05:01.\n","  Batch   600  of    837.    Elapsed: 0:05:22.\n","  Batch   640  of    837.    Elapsed: 0:05:44.\n","  Batch   680  of    837.    Elapsed: 0:06:05.\n","  Batch   720  of    837.    Elapsed: 0:06:27.\n","  Batch   760  of    837.    Elapsed: 0:06:48.\n","  Batch   800  of    837.    Elapsed: 0:07:10.\n","\n","  Average training loss: 0.00\n","  Training epcoh took: 0:07:29\n","\n","Running Validation...\n","  Accuracy: 1.0\n","  Validation took: 0:00:17\n","\n","======== Epoch 4 / 4 ========\n","Training...\n","  Batch    40  of    837.    Elapsed: 0:00:21.\n","  Batch    80  of    837.    Elapsed: 0:00:43.\n","  Batch   120  of    837.    Elapsed: 0:01:04.\n","  Batch   160  of    837.    Elapsed: 0:01:26.\n","  Batch   200  of    837.    Elapsed: 0:01:47.\n","  Batch   240  of    837.    Elapsed: 0:02:09.\n","  Batch   280  of    837.    Elapsed: 0:02:30.\n","  Batch   320  of    837.    Elapsed: 0:02:52.\n","  Batch   360  of    837.    Elapsed: 0:03:13.\n","  Batch   400  of    837.    Elapsed: 0:03:35.\n","  Batch   440  of    837.    Elapsed: 0:03:56.\n","  Batch   480  of    837.    Elapsed: 0:04:18.\n","  Batch   520  of    837.    Elapsed: 0:04:39.\n","  Batch   560  of    837.    Elapsed: 0:05:01.\n","  Batch   600  of    837.    Elapsed: 0:05:22.\n","  Batch   640  of    837.    Elapsed: 0:05:44.\n","  Batch   680  of    837.    Elapsed: 0:06:05.\n","  Batch   720  of    837.    Elapsed: 0:06:27.\n","  Batch   760  of    837.    Elapsed: 0:06:48.\n","  Batch   800  of    837.    Elapsed: 0:07:10.\n","\n","  Average training loss: 0.00\n","  Training epcoh took: 0:07:29\n","\n","Running Validation...\n","  Accuracy: 1.0\n","  Validation took: 0:00:17\n","\n","Training complete!\n","\n","======== Epoch 1 / 4 ========\n","Training...\n","  Batch    40  of    837.    Elapsed: 0:00:21.\n","  Batch    80  of    837.    Elapsed: 0:00:43.\n","  Batch   120  of    837.    Elapsed: 0:01:04.\n","  Batch   160  of    837.    Elapsed: 0:01:26.\n","  Batch   200  of    837.    Elapsed: 0:01:47.\n","  Batch   240  of    837.    Elapsed: 0:02:09.\n","  Batch   280  of    837.    Elapsed: 0:02:30.\n","  Batch   320  of    837.    Elapsed: 0:02:52.\n","  Batch   360  of    837.    Elapsed: 0:03:13.\n","  Batch   400  of    837.    Elapsed: 0:03:35.\n","  Batch   440  of    837.    Elapsed: 0:03:56.\n","  Batch   480  of    837.    Elapsed: 0:04:18.\n","  Batch   520  of    837.    Elapsed: 0:04:39.\n","  Batch   560  of    837.    Elapsed: 0:05:00.\n","  Batch   600  of    837.    Elapsed: 0:05:22.\n","  Batch   640  of    837.    Elapsed: 0:05:43.\n","  Batch   680  of    837.    Elapsed: 0:06:05.\n","  Batch   720  of    837.    Elapsed: 0:06:26.\n","  Batch   760  of    837.    Elapsed: 0:06:48.\n","  Batch   800  of    837.    Elapsed: 0:07:09.\n","\n","  Average training loss: 0.00\n","  Training epcoh took: 0:07:29\n","\n","Running Validation...\n","  Accuracy: 1.0\n","  Validation took: 0:00:17\n","\n","======== Epoch 2 / 4 ========\n","Training...\n","  Batch    40  of    837.    Elapsed: 0:00:21.\n","  Batch    80  of    837.    Elapsed: 0:00:43.\n","  Batch   120  of    837.    Elapsed: 0:01:04.\n","  Batch   160  of    837.    Elapsed: 0:01:26.\n","  Batch   200  of    837.    Elapsed: 0:01:47.\n","  Batch   240  of    837.    Elapsed: 0:02:09.\n","  Batch   280  of    837.    Elapsed: 0:02:30.\n","  Batch   320  of    837.    Elapsed: 0:02:52.\n","  Batch   360  of    837.    Elapsed: 0:03:13.\n","  Batch   400  of    837.    Elapsed: 0:03:35.\n","  Batch   440  of    837.    Elapsed: 0:03:56.\n","  Batch   480  of    837.    Elapsed: 0:04:18.\n","  Batch   520  of    837.    Elapsed: 0:04:39.\n","  Batch   560  of    837.    Elapsed: 0:05:00.\n","  Batch   600  of    837.    Elapsed: 0:05:22.\n","  Batch   640  of    837.    Elapsed: 0:05:43.\n","  Batch   680  of    837.    Elapsed: 0:06:05.\n","  Batch   720  of    837.    Elapsed: 0:06:26.\n","  Batch   760  of    837.    Elapsed: 0:06:48.\n","  Batch   800  of    837.    Elapsed: 0:07:09.\n","\n","  Average training loss: 0.00\n","  Training epcoh took: 0:07:29\n","\n","Running Validation...\n","  Accuracy: 1.0\n","  Validation took: 0:00:17\n","\n","======== Epoch 3 / 4 ========\n","Training...\n","  Batch    40  of    837.    Elapsed: 0:00:21.\n","  Batch    80  of    837.    Elapsed: 0:00:43.\n","  Batch   120  of    837.    Elapsed: 0:01:04.\n","  Batch   160  of    837.    Elapsed: 0:01:26.\n","  Batch   200  of    837.    Elapsed: 0:01:47.\n","  Batch   240  of    837.    Elapsed: 0:02:09.\n","  Batch   280  of    837.    Elapsed: 0:02:30.\n","  Batch   320  of    837.    Elapsed: 0:02:52.\n","  Batch   360  of    837.    Elapsed: 0:03:13.\n","  Batch   400  of    837.    Elapsed: 0:03:35.\n","  Batch   440  of    837.    Elapsed: 0:03:56.\n","  Batch   480  of    837.    Elapsed: 0:04:18.\n","  Batch   520  of    837.    Elapsed: 0:04:39.\n","  Batch   560  of    837.    Elapsed: 0:05:01.\n","  Batch   600  of    837.    Elapsed: 0:05:22.\n","  Batch   640  of    837.    Elapsed: 0:05:44.\n","  Batch   680  of    837.    Elapsed: 0:06:05.\n","  Batch   720  of    837.    Elapsed: 0:06:26.\n","  Batch   760  of    837.    Elapsed: 0:06:48.\n","  Batch   800  of    837.    Elapsed: 0:07:09.\n","\n","  Average training loss: 0.00\n","  Training epcoh took: 0:07:29\n","\n","Running Validation...\n","  Accuracy: 1.0\n","  Validation took: 0:00:17\n","\n","======== Epoch 4 / 4 ========\n","Training...\n","  Batch    40  of    837.    Elapsed: 0:00:21.\n","  Batch    80  of    837.    Elapsed: 0:00:43.\n","  Batch   120  of    837.    Elapsed: 0:01:04.\n","  Batch   160  of    837.    Elapsed: 0:01:26.\n","  Batch   200  of    837.    Elapsed: 0:01:47.\n","  Batch   240  of    837.    Elapsed: 0:02:09.\n","  Batch   280  of    837.    Elapsed: 0:02:30.\n","  Batch   320  of    837.    Elapsed: 0:02:52.\n","  Batch   360  of    837.    Elapsed: 0:03:13.\n","  Batch   400  of    837.    Elapsed: 0:03:35.\n","  Batch   440  of    837.    Elapsed: 0:03:56.\n","  Batch   480  of    837.    Elapsed: 0:04:18.\n","  Batch   520  of    837.    Elapsed: 0:04:39.\n","  Batch   560  of    837.    Elapsed: 0:05:00.\n","  Batch   600  of    837.    Elapsed: 0:05:22.\n","  Batch   640  of    837.    Elapsed: 0:05:43.\n","  Batch   680  of    837.    Elapsed: 0:06:05.\n","  Batch   720  of    837.    Elapsed: 0:06:26.\n","  Batch   760  of    837.    Elapsed: 0:06:48.\n","  Batch   800  of    837.    Elapsed: 0:07:09.\n","\n","  Average training loss: 0.00\n","  Training epcoh took: 0:07:29\n","\n","Running Validation...\n","  Accuracy: 1.0\n","  Validation took: 0:00:17\n","\n","Training complete!\n","\n","======== Epoch 1 / 4 ========\n","Training...\n","  Batch    40  of    837.    Elapsed: 0:00:21.\n","  Batch    80  of    837.    Elapsed: 0:00:43.\n","  Batch   120  of    837.    Elapsed: 0:01:04.\n","  Batch   160  of    837.    Elapsed: 0:01:26.\n","  Batch   200  of    837.    Elapsed: 0:01:47.\n","  Batch   240  of    837.    Elapsed: 0:02:09.\n","  Batch   280  of    837.    Elapsed: 0:02:30.\n","  Batch   320  of    837.    Elapsed: 0:02:52.\n","  Batch   360  of    837.    Elapsed: 0:03:13.\n","  Batch   400  of    837.    Elapsed: 0:03:35.\n","  Batch   440  of    837.    Elapsed: 0:03:56.\n","  Batch   480  of    837.    Elapsed: 0:04:18.\n","  Batch   520  of    837.    Elapsed: 0:04:39.\n","  Batch   560  of    837.    Elapsed: 0:05:00.\n","  Batch   600  of    837.    Elapsed: 0:05:22.\n","  Batch   640  of    837.    Elapsed: 0:05:43.\n","  Batch   680  of    837.    Elapsed: 0:06:05.\n","  Batch   720  of    837.    Elapsed: 0:06:26.\n","  Batch   760  of    837.    Elapsed: 0:06:48.\n","  Batch   800  of    837.    Elapsed: 0:07:09.\n","\n","  Average training loss: 0.00\n","  Training epcoh took: 0:07:29\n","\n","Running Validation...\n","  Accuracy: 1.0\n","  Validation took: 0:00:17\n","\n","======== Epoch 2 / 4 ========\n","Training...\n","  Batch    40  of    837.    Elapsed: 0:00:21.\n","  Batch    80  of    837.    Elapsed: 0:00:43.\n","  Batch   120  of    837.    Elapsed: 0:01:04.\n","  Batch   160  of    837.    Elapsed: 0:01:26.\n","  Batch   200  of    837.    Elapsed: 0:01:47.\n","  Batch   240  of    837.    Elapsed: 0:02:09.\n","  Batch   280  of    837.    Elapsed: 0:02:30.\n","  Batch   320  of    837.    Elapsed: 0:02:52.\n","  Batch   360  of    837.    Elapsed: 0:03:13.\n","  Batch   400  of    837.    Elapsed: 0:03:35.\n","  Batch   440  of    837.    Elapsed: 0:03:56.\n","  Batch   480  of    837.    Elapsed: 0:04:18.\n","  Batch   520  of    837.    Elapsed: 0:04:39.\n","  Batch   560  of    837.    Elapsed: 0:05:00.\n","  Batch   600  of    837.    Elapsed: 0:05:22.\n","  Batch   640  of    837.    Elapsed: 0:05:43.\n","  Batch   680  of    837.    Elapsed: 0:06:05.\n","  Batch   720  of    837.    Elapsed: 0:06:26.\n","  Batch   760  of    837.    Elapsed: 0:06:48.\n","  Batch   800  of    837.    Elapsed: 0:07:09.\n","\n","  Average training loss: 0.00\n","  Training epcoh took: 0:07:29\n","\n","Running Validation...\n","  Accuracy: 1.0\n","  Validation took: 0:00:17\n","\n","======== Epoch 3 / 4 ========\n","Training...\n","  Batch    40  of    837.    Elapsed: 0:00:21.\n","  Batch    80  of    837.    Elapsed: 0:00:43.\n","  Batch   120  of    837.    Elapsed: 0:01:04.\n","  Batch   160  of    837.    Elapsed: 0:01:26.\n","  Batch   200  of    837.    Elapsed: 0:01:47.\n","  Batch   240  of    837.    Elapsed: 0:02:09.\n","  Batch   280  of    837.    Elapsed: 0:02:30.\n","  Batch   320  of    837.    Elapsed: 0:02:52.\n","  Batch   360  of    837.    Elapsed: 0:03:13.\n","  Batch   400  of    837.    Elapsed: 0:03:35.\n","  Batch   440  of    837.    Elapsed: 0:03:56.\n","  Batch   480  of    837.    Elapsed: 0:04:18.\n","  Batch   520  of    837.    Elapsed: 0:04:39.\n","  Batch   560  of    837.    Elapsed: 0:05:01.\n","  Batch   600  of    837.    Elapsed: 0:05:22.\n","  Batch   640  of    837.    Elapsed: 0:05:43.\n","  Batch   680  of    837.    Elapsed: 0:06:05.\n","  Batch   720  of    837.    Elapsed: 0:06:26.\n","  Batch   760  of    837.    Elapsed: 0:06:48.\n","  Batch   800  of    837.    Elapsed: 0:07:09.\n","\n","  Average training loss: 0.00\n","  Training epcoh took: 0:07:29\n","\n","Running Validation...\n","  Accuracy: 1.0\n","  Validation took: 0:00:17\n","\n","======== Epoch 4 / 4 ========\n","Training...\n","  Batch    40  of    837.    Elapsed: 0:00:21.\n","  Batch    80  of    837.    Elapsed: 0:00:43.\n","  Batch   120  of    837.    Elapsed: 0:01:04.\n","  Batch   160  of    837.    Elapsed: 0:01:26.\n","  Batch   200  of    837.    Elapsed: 0:01:47.\n","  Batch   240  of    837.    Elapsed: 0:02:09.\n","  Batch   280  of    837.    Elapsed: 0:02:30.\n","  Batch   320  of    837.    Elapsed: 0:02:52.\n","  Batch   360  of    837.    Elapsed: 0:03:13.\n","  Batch   400  of    837.    Elapsed: 0:03:35.\n","  Batch   440  of    837.    Elapsed: 0:03:56.\n","  Batch   480  of    837.    Elapsed: 0:04:18.\n","  Batch   520  of    837.    Elapsed: 0:04:39.\n","  Batch   560  of    837.    Elapsed: 0:05:00.\n","  Batch   600  of    837.    Elapsed: 0:05:22.\n","  Batch   640  of    837.    Elapsed: 0:05:43.\n","  Batch   680  of    837.    Elapsed: 0:06:05.\n","  Batch   720  of    837.    Elapsed: 0:06:26.\n","  Batch   760  of    837.    Elapsed: 0:06:48.\n","  Batch   800  of    837.    Elapsed: 0:07:09.\n","\n","  Average training loss: 0.00\n","  Training epcoh took: 0:07:29\n","\n","Running Validation...\n","  Accuracy: 1.0\n","  Validation took: 0:00:17\n","\n","Training complete!\n","\n","======== Epoch 1 / 4 ========\n","Training...\n","  Batch    40  of    837.    Elapsed: 0:00:21.\n","  Batch    80  of    837.    Elapsed: 0:00:43.\n","  Batch   120  of    837.    Elapsed: 0:01:04.\n","  Batch   160  of    837.    Elapsed: 0:01:26.\n","  Batch   200  of    837.    Elapsed: 0:01:47.\n","  Batch   240  of    837.    Elapsed: 0:02:09.\n","  Batch   280  of    837.    Elapsed: 0:02:30.\n","  Batch   320  of    837.    Elapsed: 0:02:52.\n","  Batch   360  of    837.    Elapsed: 0:03:13.\n","  Batch   400  of    837.    Elapsed: 0:03:35.\n","  Batch   440  of    837.    Elapsed: 0:03:56.\n","  Batch   480  of    837.    Elapsed: 0:04:18.\n","  Batch   520  of    837.    Elapsed: 0:04:39.\n","  Batch   560  of    837.    Elapsed: 0:05:01.\n","  Batch   600  of    837.    Elapsed: 0:05:22.\n","  Batch   640  of    837.    Elapsed: 0:05:43.\n","  Batch   680  of    837.    Elapsed: 0:06:05.\n","  Batch   720  of    837.    Elapsed: 0:06:26.\n","  Batch   760  of    837.    Elapsed: 0:06:48.\n","  Batch   800  of    837.    Elapsed: 0:07:09.\n","\n","  Average training loss: 0.00\n","  Training epcoh took: 0:07:29\n","\n","Running Validation...\n","  Accuracy: 1.0\n","  Validation took: 0:00:17\n","\n","======== Epoch 2 / 4 ========\n","Training...\n","  Batch    40  of    837.    Elapsed: 0:00:21.\n","  Batch    80  of    837.    Elapsed: 0:00:43.\n","  Batch   120  of    837.    Elapsed: 0:01:04.\n","  Batch   160  of    837.    Elapsed: 0:01:26.\n","  Batch   200  of    837.    Elapsed: 0:01:47.\n","  Batch   240  of    837.    Elapsed: 0:02:09.\n","  Batch   280  of    837.    Elapsed: 0:02:30.\n","  Batch   320  of    837.    Elapsed: 0:02:52.\n","  Batch   360  of    837.    Elapsed: 0:03:13.\n","  Batch   400  of    837.    Elapsed: 0:03:35.\n","  Batch   440  of    837.    Elapsed: 0:03:56.\n","  Batch   480  of    837.    Elapsed: 0:04:18.\n","  Batch   520  of    837.    Elapsed: 0:04:39.\n","  Batch   560  of    837.    Elapsed: 0:05:01.\n","  Batch   600  of    837.    Elapsed: 0:05:22.\n","  Batch   640  of    837.    Elapsed: 0:05:43.\n","  Batch   680  of    837.    Elapsed: 0:06:05.\n","  Batch   720  of    837.    Elapsed: 0:06:26.\n","  Batch   760  of    837.    Elapsed: 0:06:48.\n","  Batch   800  of    837.    Elapsed: 0:07:09.\n","\n","  Average training loss: 0.01\n","  Training epcoh took: 0:07:29\n","\n","Running Validation...\n","  Accuracy: 1.0\n","  Validation took: 0:00:17\n","\n","======== Epoch 3 / 4 ========\n","Training...\n","  Batch    40  of    837.    Elapsed: 0:00:21.\n","  Batch    80  of    837.    Elapsed: 0:00:43.\n","  Batch   120  of    837.    Elapsed: 0:01:04.\n","  Batch   160  of    837.    Elapsed: 0:01:26.\n","  Batch   200  of    837.    Elapsed: 0:01:47.\n","  Batch   240  of    837.    Elapsed: 0:02:09.\n","  Batch   280  of    837.    Elapsed: 0:02:30.\n","  Batch   320  of    837.    Elapsed: 0:02:52.\n","  Batch   360  of    837.    Elapsed: 0:03:13.\n","  Batch   400  of    837.    Elapsed: 0:03:35.\n","  Batch   440  of    837.    Elapsed: 0:03:56.\n","  Batch   480  of    837.    Elapsed: 0:04:17.\n","  Batch   520  of    837.    Elapsed: 0:04:39.\n","  Batch   560  of    837.    Elapsed: 0:05:00.\n","  Batch   600  of    837.    Elapsed: 0:05:22.\n","  Batch   640  of    837.    Elapsed: 0:05:43.\n","  Batch   680  of    837.    Elapsed: 0:06:05.\n","  Batch   720  of    837.    Elapsed: 0:06:26.\n","  Batch   760  of    837.    Elapsed: 0:06:48.\n","  Batch   800  of    837.    Elapsed: 0:07:09.\n","\n","  Average training loss: 0.00\n","  Training epcoh took: 0:07:29\n","\n","Running Validation...\n","  Accuracy: 1.0\n","  Validation took: 0:00:17\n","\n","======== Epoch 4 / 4 ========\n","Training...\n","  Batch    40  of    837.    Elapsed: 0:00:21.\n","  Batch    80  of    837.    Elapsed: 0:00:43.\n","  Batch   120  of    837.    Elapsed: 0:01:04.\n","  Batch   160  of    837.    Elapsed: 0:01:26.\n","  Batch   200  of    837.    Elapsed: 0:01:47.\n","  Batch   240  of    837.    Elapsed: 0:02:09.\n","  Batch   280  of    837.    Elapsed: 0:02:30.\n","  Batch   320  of    837.    Elapsed: 0:02:52.\n","  Batch   360  of    837.    Elapsed: 0:03:13.\n","  Batch   400  of    837.    Elapsed: 0:03:35.\n","  Batch   440  of    837.    Elapsed: 0:03:56.\n","  Batch   480  of    837.    Elapsed: 0:04:17.\n","  Batch   520  of    837.    Elapsed: 0:04:39.\n","  Batch   560  of    837.    Elapsed: 0:05:00.\n","  Batch   600  of    837.    Elapsed: 0:05:22.\n","  Batch   640  of    837.    Elapsed: 0:05:43.\n","  Batch   680  of    837.    Elapsed: 0:06:05.\n","  Batch   720  of    837.    Elapsed: 0:06:26.\n","  Batch   760  of    837.    Elapsed: 0:06:48.\n","  Batch   800  of    837.    Elapsed: 0:07:09.\n","\n","  Average training loss: 0.00\n","  Training epcoh took: 0:07:29\n","\n","Running Validation...\n","  Accuracy: 1.0\n","  Validation took: 0:00:17\n","\n","Training complete!\n","\n","======== Epoch 1 / 4 ========\n","Training...\n","  Batch    40  of    837.    Elapsed: 0:00:21.\n","  Batch    80  of    837.    Elapsed: 0:00:43.\n","  Batch   120  of    837.    Elapsed: 0:01:04.\n","  Batch   160  of    837.    Elapsed: 0:01:26.\n","  Batch   200  of    837.    Elapsed: 0:01:47.\n","  Batch   240  of    837.    Elapsed: 0:02:09.\n","  Batch   280  of    837.    Elapsed: 0:02:30.\n","  Batch   320  of    837.    Elapsed: 0:02:52.\n","  Batch   360  of    837.    Elapsed: 0:03:13.\n","  Batch   400  of    837.    Elapsed: 0:03:35.\n","  Batch   440  of    837.    Elapsed: 0:03:56.\n","  Batch   480  of    837.    Elapsed: 0:04:18.\n","  Batch   520  of    837.    Elapsed: 0:04:39.\n","  Batch   560  of    837.    Elapsed: 0:05:01.\n","  Batch   600  of    837.    Elapsed: 0:05:22.\n","  Batch   640  of    837.    Elapsed: 0:05:43.\n","  Batch   680  of    837.    Elapsed: 0:06:05.\n","  Batch   720  of    837.    Elapsed: 0:06:26.\n","  Batch   760  of    837.    Elapsed: 0:06:48.\n","  Batch   800  of    837.    Elapsed: 0:07:09.\n","\n","  Average training loss: 0.00\n","  Training epcoh took: 0:07:29\n","\n","Running Validation...\n","  Accuracy: 1.0\n","  Validation took: 0:00:17\n","\n","======== Epoch 2 / 4 ========\n","Training...\n","  Batch    40  of    837.    Elapsed: 0:00:21.\n","  Batch    80  of    837.    Elapsed: 0:00:43.\n","  Batch   120  of    837.    Elapsed: 0:01:04.\n","  Batch   160  of    837.    Elapsed: 0:01:26.\n","  Batch   200  of    837.    Elapsed: 0:01:47.\n","  Batch   240  of    837.    Elapsed: 0:02:09.\n","  Batch   280  of    837.    Elapsed: 0:02:30.\n","  Batch   320  of    837.    Elapsed: 0:02:52.\n","  Batch   360  of    837.    Elapsed: 0:03:13.\n","  Batch   400  of    837.    Elapsed: 0:03:35.\n","  Batch   440  of    837.    Elapsed: 0:03:56.\n","  Batch   480  of    837.    Elapsed: 0:04:18.\n","  Batch   520  of    837.    Elapsed: 0:04:39.\n","  Batch   560  of    837.    Elapsed: 0:05:01.\n","  Batch   600  of    837.    Elapsed: 0:05:22.\n","  Batch   640  of    837.    Elapsed: 0:05:44.\n","  Batch   680  of    837.    Elapsed: 0:06:05.\n","  Batch   720  of    837.    Elapsed: 0:06:27.\n","  Batch   760  of    837.    Elapsed: 0:06:48.\n","  Batch   800  of    837.    Elapsed: 0:07:10.\n","\n","  Average training loss: 0.00\n","  Training epcoh took: 0:07:29\n","\n","Running Validation...\n","  Accuracy: 1.0\n","  Validation took: 0:00:17\n","\n","======== Epoch 3 / 4 ========\n","Training...\n","  Batch    40  of    837.    Elapsed: 0:00:21.\n","  Batch    80  of    837.    Elapsed: 0:00:43.\n","  Batch   120  of    837.    Elapsed: 0:01:04.\n","  Batch   160  of    837.    Elapsed: 0:01:26.\n","  Batch   200  of    837.    Elapsed: 0:01:47.\n","  Batch   240  of    837.    Elapsed: 0:02:09.\n","  Batch   280  of    837.    Elapsed: 0:02:30.\n","  Batch   320  of    837.    Elapsed: 0:02:52.\n","  Batch   360  of    837.    Elapsed: 0:03:13.\n","  Batch   400  of    837.    Elapsed: 0:03:35.\n","  Batch   440  of    837.    Elapsed: 0:03:56.\n","  Batch   480  of    837.    Elapsed: 0:04:18.\n","  Batch   520  of    837.    Elapsed: 0:04:39.\n","  Batch   560  of    837.    Elapsed: 0:05:00.\n","  Batch   600  of    837.    Elapsed: 0:05:22.\n","  Batch   640  of    837.    Elapsed: 0:05:43.\n","  Batch   680  of    837.    Elapsed: 0:06:05.\n","  Batch   720  of    837.    Elapsed: 0:06:26.\n","  Batch   760  of    837.    Elapsed: 0:06:48.\n","  Batch   800  of    837.    Elapsed: 0:07:09.\n","\n","  Average training loss: 0.00\n","  Training epcoh took: 0:07:29\n","\n","Running Validation...\n","  Accuracy: 1.0\n","  Validation took: 0:00:17\n","\n","======== Epoch 4 / 4 ========\n","Training...\n","  Batch    40  of    837.    Elapsed: 0:00:21.\n","  Batch    80  of    837.    Elapsed: 0:00:43.\n","  Batch   120  of    837.    Elapsed: 0:01:04.\n","  Batch   160  of    837.    Elapsed: 0:01:26.\n","  Batch   200  of    837.    Elapsed: 0:01:47.\n","  Batch   240  of    837.    Elapsed: 0:02:09.\n","  Batch   280  of    837.    Elapsed: 0:02:30.\n","  Batch   320  of    837.    Elapsed: 0:02:52.\n","  Batch   360  of    837.    Elapsed: 0:03:13.\n","  Batch   400  of    837.    Elapsed: 0:03:35.\n","  Batch   440  of    837.    Elapsed: 0:03:56.\n","  Batch   480  of    837.    Elapsed: 0:04:17.\n","  Batch   520  of    837.    Elapsed: 0:04:39.\n","  Batch   560  of    837.    Elapsed: 0:05:00.\n","  Batch   600  of    837.    Elapsed: 0:05:22.\n","  Batch   640  of    837.    Elapsed: 0:05:43.\n","  Batch   680  of    837.    Elapsed: 0:06:05.\n","  Batch   720  of    837.    Elapsed: 0:06:26.\n","  Batch   760  of    837.    Elapsed: 0:06:48.\n","  Batch   800  of    837.    Elapsed: 0:07:09.\n","\n","  Average training loss: 0.00\n","  Training epcoh took: 0:07:29\n","\n","Running Validation...\n","  Accuracy: 1.0\n","  Validation took: 0:00:17\n","\n","Training complete!\n","0.0005073631909807809\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZSZwn8jSvvNt","executionInfo":{"status":"ok","timestamp":1620857288910,"user_tz":240,"elapsed":1283532,"user":{"displayName":"Sydney Lister","photoUrl":"","userId":"17557786652639167110"}},"outputId":"7f073bb4-9a68-4475-b87d-6046a4b9d84b"},"source":["# This training code is based on the `run_glue.py` script here:\n","# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n","\n","\n","# Set the seed value all over the place to make this reproducible.\n","seed_val = 42\n","\n","random.seed(seed_val)\n","np.random.seed(seed_val)\n","torch.manual_seed(seed_val)\n","torch.cuda.manual_seed_all(seed_val)\n","\n","# Store the average loss after each epoch so we can plot them.\n","loss_values = []\n","\n","# For each epoch...\n","for epoch_i in range(0, epochs):\n","    \n","    # ========================================\n","    #               Training\n","    # ========================================\n","    \n","    # Perform one full pass over the training set.\n","\n","    print(\"\")\n","    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n","    print('Training...')\n","\n","    # Measure how long the training epoch takes.\n","    t0 = time.time()\n","\n","    # Reset the total loss for this epoch.\n","    total_loss = 0\n","\n","    # Put the model into training mode. Don't be mislead--the call to \n","    # `train` just changes the *mode*, it doesn't *perform* the training.\n","    # `dropout` and `batchnorm` layers behave differently during training\n","    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n","    model.train()\n","\n","    # For each batch of training data...\n","    for step, batch in enumerate(train_dataloader):\n","\n","        # Progress update every 40 batches.\n","        if step % 40 == 0 and not step == 0:\n","            # Calculate elapsed time in minutes.\n","            elapsed = format_time(time.time() - t0)\n","            \n","            # Report progress.\n","            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n","\n","        # Unpack this training batch from our dataloader. \n","        #\n","        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n","        # `to` method.\n","        #\n","        # `batch` contains three pytorch tensors:\n","        #   [0]: input ids \n","        #   [1]: attention masks\n","        #   [2]: labels \n","        b_input_ids = batch[0].to(device)\n","        b_input_mask = batch[1].to(device)\n","        b_labels = batch[2].to(device)\n","        # b_labels.type(torch.LongTensor)\n","        # print(b_labels.type())\n","        b_labels = b_labels.type(torch.cuda.LongTensor)\n","        # b_labels = Variable(b_labels_temp, requires_grad=True).cuda()\n","\n","        # Always clear any previously calculated gradients before performing a\n","        # backward pass. PyTorch doesn't do this automatically because \n","        # accumulating the gradients is \"convenient while training RNNs\". \n","        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n","        model.zero_grad()        \n","\n","        # Perform a forward pass (evaluate the model on this training batch).\n","        # This will return the loss (rather than the model output) because we\n","        # have provided the `labels`.\n","        # The documentation for this `model` function is here: \n","        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n","        outputs = model(b_input_ids, \n","                    token_type_ids=None, \n","                    attention_mask=b_input_mask, \n","                    labels=b_labels)\n","        \n","        # The call to `model` always returns a tuple, so we need to pull the \n","        # loss value out of the tuple.\n","        loss = outputs[0]\n","\n","        # Accumulate the training loss over all of the batches so that we can\n","        # calculate the average loss at the end. `loss` is a Tensor containing a\n","        # single value; the `.item()` function just returns the Python value \n","        # from the tensor.\n","        total_loss += loss.item()\n","\n","        # Perform a backward pass to calculate the gradients.\n","        loss.backward()\n","\n","        # Clip the norm of the gradients to 1.0.\n","        # This is to help prevent the \"exploding gradients\" problem.\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","\n","        # Update parameters and take a step using the computed gradient.\n","        # The optimizer dictates the \"update rule\"--how the parameters are\n","        # modified based on their gradients, the learning rate, etc.\n","        optimizer.step()\n","\n","        # Update the learning rate.\n","        scheduler.step()\n","\n","    # Calculate the average loss over the training data.\n","    avg_train_loss = total_loss / len(train_dataloader)            \n","    \n","    # Store the loss value for plotting the learning curve.\n","    loss_values.append(avg_train_loss)\n","\n","    print(\"\")\n","    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n","    print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n","        \n","    # ========================================\n","    #               Validation\n","    # ========================================\n","    # After the completion of each training epoch, measure our performance on\n","    # our validation set.\n","\n","    print(\"\")\n","    print(\"Running Validation...\")\n","\n","    t0 = time.time()\n","\n","    # Put the model in evaluation mode--the dropout layers behave differently\n","    # during evaluation.\n","    model.eval()\n","\n","    # Tracking variables \n","    eval_loss, eval_accuracy = 0, 0\n","    nb_eval_steps, nb_eval_examples = 0, 0\n","\n","    # Evaluate data for one epoch\n","    for batch in validation_dataloader:\n","        \n","        # Add batch to GPU\n","        batch = tuple(t.to(device) for t in batch)\n","        \n","        # Unpack the inputs from our dataloader\n","        b_input_ids, b_input_mask, b_labels = batch\n","        \n","        # Telling the model not to compute or store gradients, saving memory and\n","        # speeding up validation\n","        with torch.no_grad():        \n","\n","            # Forward pass, calculate logit predictions.\n","            # This will return the logits rather than the loss because we have\n","            # not provided labels.\n","            # token_type_ids is the same as the \"segment ids\", which \n","            # differentiates sentence 1 and 2 in 2-sentence tasks.\n","            # The documentation for this `model` function is here: \n","            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n","            outputs = model(b_input_ids, \n","                            token_type_ids=None, \n","                            attention_mask=b_input_mask)\n","        \n","        # Get the \"logits\" output by the model. The \"logits\" are the output\n","        # values prior to applying an activation function like the softmax.\n","        logits = outputs[0]\n","\n","        # Move logits and labels to CPU\n","        logits = logits.detach().cpu().numpy()\n","        label_ids = b_labels.to('cpu').numpy()\n","        \n","        # Calculate the accuracy for this batch of test sentences.\n","        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n","        \n","        # Accumulate the total accuracy.\n","        eval_accuracy += tmp_eval_accuracy\n","\n","        # Track the number of batches\n","        nb_eval_steps += 1\n","\n","    # Report the final accuracy for this validation run.\n","    print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n","    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n","\n","print(\"\")\n","print(\"Training complete!\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\n","======== Epoch 1 / 4 ========\n","Training...\n","  Batch    40  of    837.    Elapsed: 0:00:15.\n","  Batch    80  of    837.    Elapsed: 0:00:30.\n","  Batch   120  of    837.    Elapsed: 0:00:44.\n","  Batch   160  of    837.    Elapsed: 0:00:59.\n","  Batch   200  of    837.    Elapsed: 0:01:14.\n","  Batch   240  of    837.    Elapsed: 0:01:29.\n","  Batch   280  of    837.    Elapsed: 0:01:44.\n","  Batch   320  of    837.    Elapsed: 0:01:58.\n","  Batch   360  of    837.    Elapsed: 0:02:13.\n","  Batch   400  of    837.    Elapsed: 0:02:28.\n","  Batch   440  of    837.    Elapsed: 0:02:43.\n","  Batch   480  of    837.    Elapsed: 0:02:58.\n","  Batch   520  of    837.    Elapsed: 0:03:12.\n","  Batch   560  of    837.    Elapsed: 0:03:27.\n","  Batch   600  of    837.    Elapsed: 0:03:42.\n","  Batch   640  of    837.    Elapsed: 0:03:57.\n","  Batch   680  of    837.    Elapsed: 0:04:12.\n","  Batch   720  of    837.    Elapsed: 0:04:26.\n","  Batch   760  of    837.    Elapsed: 0:04:41.\n","  Batch   800  of    837.    Elapsed: 0:04:56.\n","\n","  Average training loss: 0.08\n","  Training epcoh took: 0:05:09\n","\n","Running Validation...\n","  Accuracy: 0.99\n","  Validation took: 0:00:11\n","\n","======== Epoch 2 / 4 ========\n","Training...\n","  Batch    40  of    837.    Elapsed: 0:00:15.\n","  Batch    80  of    837.    Elapsed: 0:00:30.\n","  Batch   120  of    837.    Elapsed: 0:00:44.\n","  Batch   160  of    837.    Elapsed: 0:00:59.\n","  Batch   200  of    837.    Elapsed: 0:01:14.\n","  Batch   240  of    837.    Elapsed: 0:01:29.\n","  Batch   280  of    837.    Elapsed: 0:01:44.\n","  Batch   320  of    837.    Elapsed: 0:01:59.\n","  Batch   360  of    837.    Elapsed: 0:02:13.\n","  Batch   400  of    837.    Elapsed: 0:02:28.\n","  Batch   440  of    837.    Elapsed: 0:02:43.\n","  Batch   480  of    837.    Elapsed: 0:02:58.\n","  Batch   520  of    837.    Elapsed: 0:03:13.\n","  Batch   560  of    837.    Elapsed: 0:03:27.\n","  Batch   600  of    837.    Elapsed: 0:03:42.\n","  Batch   640  of    837.    Elapsed: 0:03:57.\n","  Batch   680  of    837.    Elapsed: 0:04:12.\n","  Batch   720  of    837.    Elapsed: 0:04:27.\n","  Batch   760  of    837.    Elapsed: 0:04:41.\n","  Batch   800  of    837.    Elapsed: 0:04:56.\n","\n","  Average training loss: 0.03\n","  Training epcoh took: 0:05:10\n","\n","Running Validation...\n","  Accuracy: 0.99\n","  Validation took: 0:00:11\n","\n","======== Epoch 3 / 4 ========\n","Training...\n","  Batch    40  of    837.    Elapsed: 0:00:15.\n","  Batch    80  of    837.    Elapsed: 0:00:30.\n","  Batch   120  of    837.    Elapsed: 0:00:44.\n","  Batch   160  of    837.    Elapsed: 0:00:59.\n","  Batch   200  of    837.    Elapsed: 0:01:14.\n","  Batch   240  of    837.    Elapsed: 0:01:29.\n","  Batch   280  of    837.    Elapsed: 0:01:44.\n","  Batch   320  of    837.    Elapsed: 0:01:58.\n","  Batch   360  of    837.    Elapsed: 0:02:13.\n","  Batch   400  of    837.    Elapsed: 0:02:28.\n","  Batch   440  of    837.    Elapsed: 0:02:43.\n","  Batch   480  of    837.    Elapsed: 0:02:58.\n","  Batch   520  of    837.    Elapsed: 0:03:12.\n","  Batch   560  of    837.    Elapsed: 0:03:27.\n","  Batch   600  of    837.    Elapsed: 0:03:42.\n","  Batch   640  of    837.    Elapsed: 0:03:57.\n","  Batch   680  of    837.    Elapsed: 0:04:11.\n","  Batch   720  of    837.    Elapsed: 0:04:26.\n","  Batch   760  of    837.    Elapsed: 0:04:41.\n","  Batch   800  of    837.    Elapsed: 0:04:56.\n","\n","  Average training loss: 0.01\n","  Training epcoh took: 0:05:09\n","\n","Running Validation...\n","  Accuracy: 0.99\n","  Validation took: 0:00:11\n","\n","======== Epoch 4 / 4 ========\n","Training...\n","  Batch    40  of    837.    Elapsed: 0:00:15.\n","  Batch    80  of    837.    Elapsed: 0:00:30.\n","  Batch   120  of    837.    Elapsed: 0:00:44.\n","  Batch   160  of    837.    Elapsed: 0:00:59.\n","  Batch   200  of    837.    Elapsed: 0:01:14.\n","  Batch   240  of    837.    Elapsed: 0:01:29.\n","  Batch   280  of    837.    Elapsed: 0:01:44.\n","  Batch   320  of    837.    Elapsed: 0:01:58.\n","  Batch   360  of    837.    Elapsed: 0:02:13.\n","  Batch   400  of    837.    Elapsed: 0:02:28.\n","  Batch   440  of    837.    Elapsed: 0:02:43.\n","  Batch   480  of    837.    Elapsed: 0:02:58.\n","  Batch   520  of    837.    Elapsed: 0:03:12.\n","  Batch   560  of    837.    Elapsed: 0:03:27.\n","  Batch   600  of    837.    Elapsed: 0:03:42.\n","  Batch   640  of    837.    Elapsed: 0:03:57.\n","  Batch   680  of    837.    Elapsed: 0:04:12.\n","  Batch   720  of    837.    Elapsed: 0:04:27.\n","  Batch   760  of    837.    Elapsed: 0:04:41.\n","  Batch   800  of    837.    Elapsed: 0:04:56.\n","\n","  Average training loss: 0.00\n","  Training epcoh took: 0:05:10\n","\n","Running Validation...\n","  Accuracy: 0.99\n","  Validation took: 0:00:11\n","\n","Training complete!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ZQZxsbDBaXCM"},"source":["avg acuracy = 70.775"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":268},"id":"WPhLztqoXjlT","executionInfo":{"status":"ok","timestamp":1620857604150,"user_tz":240,"elapsed":527,"user":{"displayName":"Sydney Lister","photoUrl":"","userId":"17557786652639167110"}},"outputId":"e81d195c-cd49-4784-bca6-9fe53cd2133f"},"source":["import matplotlib.pyplot as plt\n","\n","plt.plot(loss_values)\n","plt.show()"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"code","metadata":{"id":"_60fGCI8N7oJ"},"source":["torch.cuda.empty_cache()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0LhOWTisLMLL","executionInfo":{"elapsed":7972,"status":"ok","timestamp":1611844200256,"user":{"displayName":"Sydney Lister","photoUrl":"","userId":"17557786652639167110"},"user_tz":300},"outputId":"e2b98b69-1791-43ea-c090-653313be8030"},"source":["model_path = os.getcwd() + '/Model/'\n","model.save_pretrained(model_path)\n","tokenizer.save_pretrained(model_path)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["('/content/gdrive/My Drive/ML Research ListerVosoughi/SMIP/Data/Model/tokenizer_config.json',\n"," '/content/gdrive/My Drive/ML Research ListerVosoughi/SMIP/Data/Model/special_tokens_map.json',\n"," '/content/gdrive/My Drive/ML Research ListerVosoughi/SMIP/Data/Model/vocab.txt',\n"," '/content/gdrive/My Drive/ML Research ListerVosoughi/SMIP/Data/Model/added_tokens.json')"]},"metadata":{"tags":[]},"execution_count":38}]},{"cell_type":"markdown","metadata":{"id":"3qg7gSRo_9iz"},"source":["## **Predicting**"]},{"cell_type":"code","metadata":{"id":"e_3lfLvo2DOz"},"source":["model = BertForSequenceClassification.from_pretrained(os.getcwd() + '/Model/', \n","    num_labels = 2, # The number of output labels--2 for binary classification.   \n","    output_attentions = False, # Whether the model returns attentions weights.\n","    output_hidden_states = False, # Whether the model returns all hidden-states.\n","    ) # Assumes already in Data folder \n","tokenizer = BertTokenizer.from_pretrained(os.getcwd() + '/Model/', do_lower_case=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"f_U7-_AE2_nL"},"source":["df = pd.read_csv(os.getcwd() + '/all_tweets.tsv', low_memory=False, sep='\\t')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HjeQ9YlH4xpH"},"source":["tweets = df.text.values"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"P3LjV7hZO0y6"},"source":["print(len(tweets))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XgLha6sh4uM2","executionInfo":{"elapsed":9468726,"status":"ok","timestamp":1614182489503,"user":{"displayName":"Sydney Lister","photoUrl":"","userId":"17557786652639167110"},"user_tz":300},"outputId":"f9d033ee-8aa4-459d-f65f-1a172302b425"},"source":["# Tokenize all of the tweet and map the tokens to thier word IDs.\n","test_ids = []\n","\n","# For every tweet...\n","for tweet in tweets:\n","    # `encode` will:\n","    #   (1) Tokenize the tweet.\n","    #   (2) Prepend the `[CLS]` token to the start.\n","    #   (3) Append the `[SEP]` token to the end.\n","    #   (4) Map tokens to their IDs.\n","    if tweet is np.nan: \n","      # test_ids.append('')\n","      tweet = ''\n","    encoded_tweet = tokenizer.encode(\n","                        tweet,                      # Tweet to encode.\n","                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n","                        # This function also supports truncation and conversion\n","                        # to pytorch tensors, but we need to do padding, so we\n","                        # can't use these features :( .\n","                        # max_length = 128,          # Truncate all tweets.\n","                        #return_tensors = 'pt',     # Return pytorch tensors.\n","                   )\n","      # print(encoded_tweet)\n","      # Add the encoded tweet to the list.\n","    test_ids.append(encoded_tweet)\n","# f = np.savetxt(os.getcwd() + '/encoded_tweets.txt', np.array(test_ids, dtype=object), fmt='%.18s', delimiter=', ')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Token indices sequence length is longer than the specified maximum sequence length for this model (668 > 512). Running this sequence through the model will result in indexing errors\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"hPdVP_ahEVgF"},"source":["with open(os.getcwd() + '/encoded_tweets.txt', mode='w') as out_file:\n","  for id in test_ids: \n","    out_file.write(str(id) + '\\n') \n","out_file.close()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pzT2BoLBDsOf"},"source":["f = np.savetxt(os.getcwd() + '/encoded_tweets.txt', np.array(test_ids, dtype=object), fmt='%.18s', delimiter=', ')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ok5mMTjcfn6J"},"source":["# test_ids = np.loadtxt(os.getcwd() + '/encoded_tweets.txt', delimiter=', ', ndmin=1, dtype=object)\n","with open(os.getcwd() + '/encoded_tweets.txt', mode='r') as in_file:\n","  test_ids = []\n","  batch_size = 2000000\n","  j = 1 # number of times ive run it, needs to be 6 in end\n","  for i, line in enumerate(in_file):\n","    if i >= j*batch_size and i < (j+1)*batch_size: \n","      l = line.replace('[', '').replace(']','').split(', ')\n","      k = [int(s.strip()) for s in l]\n","      test_ids.append(k)\n","    elif i >= (j+1)*batch_size: break\n","in_file.close()\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wlmnST860QF1","executionInfo":{"elapsed":517,"status":"ok","timestamp":1614216784228,"user":{"displayName":"Sydney Lister","photoUrl":"","userId":"17557786652639167110"},"user_tz":300},"outputId":"2be3fc28-37bd-43aa-f47d-9d6cb85d1d77"},"source":["print(len(test_ids)/10)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["1330418.9\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"H_NAKkYow7VV"},"source":["size = int(len(test_ids)/16)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eRJDrkp_XDsQ","executionInfo":{"elapsed":1213968,"status":"ok","timestamp":1614262612022,"user":{"displayName":"Sydney Lister","photoUrl":"","userId":"17557786652639167110"},"user_tz":300},"outputId":"f19b85d9-2b54-469d-d487-16c8916d8793"},"source":["# Set the maximum sequence length.\n","# I've chosen 400 somewhat arbitrarily. It's slightly larger than the\n","# maximum training tweet length of 357...\n","MAX_LEN = 400\n","\n","print('\\nPadding/truncating all sentences to %d values...' % MAX_LEN)\n","\n","print('\\nPadding token: \"{:}\", ID: {:}'.format(tokenizer.pad_token, tokenizer.pad_token_id))\n","\n","# Pad our input tokens with value 0.\n","# \"post\" indicates that we want to pad and truncate at the end of the sequence,\n","# as opposed to the beginning.\n","# test_ids = pad_sequences(test_ids, maxlen=MAX_LEN, dtype=\"long\", \n","#                           value=0, truncating=\"post\", padding=\"post\")\n","with open(os.getcwd() + '/padded_tweets.txt', mode='a') as out_file: \n","  for i in range(16):\n","    print('Padding batch ', i)\n","    dynamic_padding = len(max(test_ids[i*size:(i+1)*size], key=len)) # https://towardsdatascience.com/divide-hugging-face-transformers-training-time-by-2-or-more-21bf7129db9q-21bf7129db9e\n","    print(dynamic_padding)\n","    padded_test_ids = pad_sequences(test_ids[i*size:(i+1)*size], maxlen=dynamic_padding, dtype=\"long\",  value=0, truncating=\"post\", padding=\"post\")\n","    out_file.write('\\n'.join(str(i) for i in padded_test_ids))\n","    print('Done padding batch ', i)\n","\n","  # test_ids_1 = pad_sequences(test_ids[:size], maxlen=MAX_LEN, dtype=\"long\",  value=0, truncating=\"post\", padding=\"post\")\n","  print('\\nDone.')\n","out_file.close()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\n","Padding/truncating all sentences to 400 values...\n","\n","Padding token: \"[PAD]\", ID: 0\n","Padding batch  0\n","1015\n","Done padding batch  0\n","Padding batch  1\n","1480\n","Done padding batch  1\n","Padding batch  2\n","749\n","Done padding batch  2\n","Padding batch  3\n","795\n","Done padding batch  3\n","Padding batch  4\n","716\n","Done padding batch  4\n","Padding batch  5\n","693\n","Done padding batch  5\n","Padding batch  6\n","1000\n","Done padding batch  6\n","Padding batch  7\n","1088\n","Done padding batch  7\n","Padding batch  8\n","350\n","Done padding batch  8\n","Padding batch  9\n","354\n","Done padding batch  9\n","Padding batch  10\n","345\n","Done padding batch  10\n","Padding batch  11\n","350\n","Done padding batch  11\n","Padding batch  12\n","370\n","Done padding batch  12\n","Padding batch  13\n","356\n","Done padding batch  13\n","Padding batch  14\n","360\n","Done padding batch  14\n","Padding batch  15\n","352\n","Done padding batch  15\n","\n","Done.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"3QTRiZEqLWuX"},"source":["# test_ids = np.loadtxt(os.getcwd() + '/encoded_tweets.txt', delimiter=', ', ndmin=1, dtype=object)\n","with open(os.getcwd() + '/encoded_tweets.txt', mode='r') as in_file:\n","  test_ids = []\n","  batch_size = 2000000\n","  j = 2 # number of times ive run it, needs to be 6 in end\n","  for i, line in enumerate(in_file):\n","    if i >= j*batch_size and i < (j+1)*batch_size: \n","      l = line.replace('[', '').replace(']','').split(', ')\n","      k = [int(s.strip()) for s in l]\n","      test_ids.append(k)\n","    elif i >= (j+1)*batch_size: break\n","in_file.close()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fvJGhjYpMFD5"},"source":["size = int(len(test_ids)/16)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yMBgIs4PLn4l","executionInfo":{"elapsed":1064958,"status":"ok","timestamp":1614263677651,"user":{"displayName":"Sydney Lister","photoUrl":"","userId":"17557786652639167110"},"user_tz":300},"outputId":"217b0922-12c1-406e-fd2b-7af2c46850ff"},"source":["# Set the maximum sequence length.\n","# I've chosen 400 somewhat arbitrarily. It's slightly larger than the\n","# maximum training tweet length of 357...\n","MAX_LEN = 400\n","\n","print('\\nPadding/truncating all sentences to %d values...' % MAX_LEN)\n","\n","print('\\nPadding token: \"{:}\", ID: {:}'.format(tokenizer.pad_token, tokenizer.pad_token_id))\n","\n","# Pad our input tokens with value 0.\n","# \"post\" indicates that we want to pad and truncate at the end of the sequence,\n","# as opposed to the beginning.\n","# test_ids = pad_sequences(test_ids, maxlen=MAX_LEN, dtype=\"long\", \n","#                           value=0, truncating=\"post\", padding=\"post\")\n","with open(os.getcwd() + '/padded_tweets.txt', mode='a') as out_file: \n","  for i in range(16):\n","    print('Padding batch ', i)\n","    dynamic_padding = len(max(test_ids[i*size:(i+1)*size], key=len)) # https://towardsdatascience.com/divide-hugging-face-transformers-training-time-by-2-or-more-21bf7129db9q-21bf7129db9e\n","    print(dynamic_padding)\n","    padded_test_ids = pad_sequences(test_ids[i*size:(i+1)*size], maxlen=dynamic_padding, dtype=\"long\",  value=0, truncating=\"post\", padding=\"post\")\n","    out_file.write('\\n'.join(str(i) for i in padded_test_ids))\n","    print('Done padding batch ', i)\n","\n","  # test_ids_1 = pad_sequences(test_ids[:size], maxlen=MAX_LEN, dtype=\"long\",  value=0, truncating=\"post\", padding=\"post\")\n","  print('\\nDone.')\n","out_file.close()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\n","Padding/truncating all sentences to 400 values...\n","\n","Padding token: \"[PAD]\", ID: 0\n","Padding batch  0\n","335\n","Done padding batch  0\n","Padding batch  1\n","357\n","Done padding batch  1\n","Padding batch  2\n","340\n","Done padding batch  2\n","Padding batch  3\n","349\n","Done padding batch  3\n","Padding batch  4\n","339\n","Done padding batch  4\n","Padding batch  5\n","329\n","Done padding batch  5\n","Padding batch  6\n","350\n","Done padding batch  6\n","Padding batch  7\n","341\n","Done padding batch  7\n","Padding batch  8\n","354\n","Done padding batch  8\n","Padding batch  9\n","345\n","Done padding batch  9\n","Padding batch  10\n","353\n","Done padding batch  10\n","Padding batch  11\n","346\n","Done padding batch  11\n","Padding batch  12\n","363\n","Done padding batch  12\n","Padding batch  13\n","359\n","Done padding batch  13\n","Padding batch  14\n","372\n","Done padding batch  14\n","Padding batch  15\n","406\n","Done padding batch  15\n","\n","Done.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"yxWkqlaDLSph"},"source":["# test_ids = np.loadtxt(os.getcwd() + '/encoded_tweets.txt', delimiter=', ', ndmin=1, dtype=object)\n","with open(os.getcwd() + '/encoded_tweets.txt', mode='r') as in_file:\n","  test_ids = []\n","  batch_size = 2000000\n","  j = 3 # number of times ive run it, needs to be 6 in end\n","  for i, line in enumerate(in_file):\n","    if i >= j*batch_size and i < (j+1)*batch_size: \n","      l = line.replace('[', '').replace(']','').split(', ')\n","      k = [int(s.strip()) for s in l]\n","      test_ids.append(k)\n","    elif i >= (j+1)*batch_size: break\n","in_file.close()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1cVJF9MVLop5","executionInfo":{"elapsed":1060842,"status":"ok","timestamp":1614264738942,"user":{"displayName":"Sydney Lister","photoUrl":"","userId":"17557786652639167110"},"user_tz":300},"outputId":"c8ed9e98-0df3-4c39-a72c-3ba3a9f59cb3"},"source":["# Set the maximum sequence length.\n","# I've chosen 400 somewhat arbitrarily. It's slightly larger than the\n","# maximum training tweet length of 357...\n","MAX_LEN = 400\n","\n","print('\\nPadding/truncating all sentences to %d values...' % MAX_LEN)\n","\n","print('\\nPadding token: \"{:}\", ID: {:}'.format(tokenizer.pad_token, tokenizer.pad_token_id))\n","\n","# Pad our input tokens with value 0.\n","# \"post\" indicates that we want to pad and truncate at the end of the sequence,\n","# as opposed to the beginning.\n","# test_ids = pad_sequences(test_ids, maxlen=MAX_LEN, dtype=\"long\", \n","#                           value=0, truncating=\"post\", padding=\"post\")\n","with open(os.getcwd() + '/padded_tweets.txt', mode='a') as out_file: \n","  for i in range(16):\n","    print('Padding batch ', i)\n","    dynamic_padding = len(max(test_ids[i*size:(i+1)*size], key=len)) # https://towardsdatascience.com/divide-hugging-face-transformers-training-time-by-2-or-more-21bf7129db9q-21bf7129db9e\n","    print(dynamic_padding)\n","    if dynamic_padding > MAX_LEN: \n","      dynamic_padding = MAX_LEN\n","    padded_test_ids = pad_sequences(test_ids[i*size:(i+1)*size], maxlen=dynamic_padding, dtype=\"long\",  value=0, truncating=\"post\", padding=\"post\")\n","    out_file.write('\\n'.join(str(i) for i in padded_test_ids))\n","    print('Done padding batch ', i)\n","\n","  # test_ids_1 = pad_sequences(test_ids[:size], maxlen=MAX_LEN, dtype=\"long\",  value=0, truncating=\"post\", padding=\"post\")\n","  print('\\nDone.')\n","out_file.close()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\n","Padding/truncating all sentences to 400 values...\n","\n","Padding token: \"[PAD]\", ID: 0\n","Padding batch  0\n","405\n","Done padding batch  0\n","Padding batch  1\n","351\n","Done padding batch  1\n","Padding batch  2\n","356\n","Done padding batch  2\n","Padding batch  3\n","412\n","Done padding batch  3\n","Padding batch  4\n","362\n","Done padding batch  4\n","Padding batch  5\n","339\n","Done padding batch  5\n","Padding batch  6\n","362\n","Done padding batch  6\n","Padding batch  7\n","406\n","Done padding batch  7\n","Padding batch  8\n","353\n","Done padding batch  8\n","Padding batch  9\n","331\n","Done padding batch  9\n","Padding batch  10\n","349\n","Done padding batch  10\n","Padding batch  11\n","343\n","Done padding batch  11\n","Padding batch  12\n","353\n","Done padding batch  12\n","Padding batch  13\n","375\n","Done padding batch  13\n","Padding batch  14\n","348\n","Done padding batch  14\n","Padding batch  15\n","343\n","Done padding batch  15\n","\n","Done.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"7sDB-wjbLZs1"},"source":["# test_ids = np.loadtxt(os.getcwd() + '/encoded_tweets.txt', delimiter=', ', ndmin=1, dtype=object)\n","with open(os.getcwd() + '/encoded_tweets.txt', mode='r') as in_file:\n","  test_ids = []\n","  batch_size = 2000000\n","  j = 4 # number of times ive run it, needs to be 6 in end\n","  for i, line in enumerate(in_file):\n","    if i >= j*batch_size and i < (j+1)*batch_size: \n","      l = line.replace('[', '').replace(']','').split(', ')\n","      k = [int(s.strip()) for s in l]\n","      test_ids.append(k)\n","    elif i >= (j+1)*batch_size: break\n","in_file.close()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_ACW5Py7Lqs_","executionInfo":{"elapsed":2170827,"status":"ok","timestamp":1614265848939,"user":{"displayName":"Sydney Lister","photoUrl":"","userId":"17557786652639167110"},"user_tz":300},"outputId":"82fb5229-6a4c-439e-f871-5852958f3353"},"source":["# Set the maximum sequence length.\n","# I've chosen 400 somewhat arbitrarily. It's slightly larger than the\n","# maximum training tweet length of 357...\n","MAX_LEN = 400\n","\n","print('\\nPadding/truncating all sentences to %d values...' % MAX_LEN)\n","\n","print('\\nPadding token: \"{:}\", ID: {:}'.format(tokenizer.pad_token, tokenizer.pad_token_id))\n","\n","# Pad our input tokens with value 0.\n","# \"post\" indicates that we want to pad and truncate at the end of the sequence,\n","# as opposed to the beginning.\n","# test_ids = pad_sequences(test_ids, maxlen=MAX_LEN, dtype=\"long\", \n","#                           value=0, truncating=\"post\", padding=\"post\")\n","with open(os.getcwd() + '/padded_tweets.txt', mode='a') as out_file: \n","  for i in range(16):\n","    print('Padding batch ', i)\n","    dynamic_padding = len(max(test_ids[i*size:(i+1)*size], key=len)) # https://towardsdatascience.com/divide-hugging-face-transformers-training-time-by-2-or-more-21bf7129db9q-21bf7129db9e\n","    if dynamic_padding > MAX_LEN: \n","      dynamic_padding = MAX_LEN\n","    padded_test_ids = pad_sequences(test_ids[i*size:(i+1)*size], maxlen=dynamic_padding, dtype=\"long\",  value=0, truncating=\"post\", padding=\"post\")\n","    out_file.write('\\n'.join(str(i) for i in padded_test_ids))\n","    print('Done padding batch ', i)\n","\n","  # test_ids_1 = pad_sequences(test_ids[:size], maxlen=MAX_LEN, dtype=\"long\",  value=0, truncating=\"post\", padding=\"post\")\n","  print('\\nDone.')\n","out_file.close()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\n","Padding/truncating all sentences to 400 values...\n","\n","Padding token: \"[PAD]\", ID: 0\n","Padding batch  0\n","375\n","Done padding batch  0\n","Padding batch  1\n","376\n","Done padding batch  1\n","Padding batch  2\n","362\n","Done padding batch  2\n","Padding batch  3\n","373\n","Done padding batch  3\n","Padding batch  4\n","349\n","Done padding batch  4\n","Padding batch  5\n","349\n","Done padding batch  5\n","Padding batch  6\n","370\n","Done padding batch  6\n","Padding batch  7\n","394\n","Done padding batch  7\n","Padding batch  8\n","365\n","Done padding batch  8\n","Padding batch  9\n","484\n","Done padding batch  9\n","Padding batch  10\n","377\n","Done padding batch  10\n","Padding batch  11\n","392\n","Done padding batch  11\n","Padding batch  12\n","366\n","Done padding batch  12\n","Padding batch  13\n","351\n","Done padding batch  13\n","Padding batch  14\n","369\n","Done padding batch  14\n","Padding batch  15\n","400\n","Done padding batch  15\n","\n","Done.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"NHOc49DDLa4J"},"source":["# test_ids = np.loadtxt(os.getcwd() + '/encoded_tweets.txt', delimiter=', ', ndmin=1, dtype=object)\n","with open(os.getcwd() + '/encoded_tweets.txt', mode='r') as in_file:\n","  test_ids = []\n","  batch_size = 2000000\n","  j = 5 # number of times ive run it, needs to be 6 in end\n","  for i, line in enumerate(in_file):\n","    if i >= j*batch_size and i < (j+1)*batch_size: \n","      l = line.replace('[', '').replace(']','').split(', ')\n","      k = [int(s.strip()) for s in l]\n","      test_ids.append(k)\n","    elif i >= (j+1)*batch_size: break\n","in_file.close()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"msn41rSCLrs3","executionInfo":{"elapsed":3262226,"status":"ok","timestamp":1614266940347,"user":{"displayName":"Sydney Lister","photoUrl":"","userId":"17557786652639167110"},"user_tz":300},"outputId":"809296fc-f526-41c1-9ebf-4386edd57c4f"},"source":["# Set the maximum sequence length.\n","# I've chosen 400 somewhat arbitrarily. It's slightly larger than the\n","# maximum training tweet length of 357...\n","MAX_LEN = 400\n","\n","print('\\nPadding/truncating all sentences to %d values...' % MAX_LEN)\n","\n","print('\\nPadding token: \"{:}\", ID: {:}'.format(tokenizer.pad_token, tokenizer.pad_token_id))\n","\n","# Pad our input tokens with value 0.\n","# \"post\" indicates that we want to pad and truncate at the end of the sequence,\n","# as opposed to the beginning.\n","# test_ids = pad_sequences(test_ids, maxlen=MAX_LEN, dtype=\"long\", \n","#                           value=0, truncating=\"post\", padding=\"post\")\n","with open(os.getcwd() + '/padded_tweets.txt', mode='a') as out_file: \n","  for i in range(16):\n","    print('Padding batch ', i)\n","    dynamic_padding = len(max(test_ids[i*size:(i+1)*size], key=len)) # https://towardsdatascience.com/divide-hugging-face-transformers-training-time-by-2-or-more-21bf7129db9q-21bf7129db9e\n","    if dynamic_padding > MAX_LEN: \n","      dynamic_padding = MAX_LEN\n","    padded_test_ids = pad_sequences(test_ids[i*size:(i+1)*size], maxlen=dynamic_padding, dtype=\"long\",  value=0, truncating=\"post\", padding=\"post\")\n","    out_file.write('\\n'.join(str(i) for i in padded_test_ids))\n","    print('Done padding batch ', i)\n","\n","  # test_ids_1 = pad_sequences(test_ids[:size], maxlen=MAX_LEN, dtype=\"long\",  value=0, truncating=\"post\", padding=\"post\")\n","  print('\\nDone.')\n","out_file.close()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\n","Padding/truncating all sentences to 400 values...\n","\n","Padding token: \"[PAD]\", ID: 0\n","Padding batch  0\n","359\n","Done padding batch  0\n","Padding batch  1\n","354\n","Done padding batch  1\n","Padding batch  2\n","393\n","Done padding batch  2\n","Padding batch  3\n","359\n","Done padding batch  3\n","Padding batch  4\n","349\n","Done padding batch  4\n","Padding batch  5\n","367\n","Done padding batch  5\n","Padding batch  6\n","337\n","Done padding batch  6\n","Padding batch  7\n","353\n","Done padding batch  7\n","Padding batch  8\n","357\n","Done padding batch  8\n","Padding batch  9\n","342\n","Done padding batch  9\n","Padding batch  10\n","370\n","Done padding batch  10\n","Padding batch  11\n","381\n","Done padding batch  11\n","Padding batch  12\n","347\n","Done padding batch  12\n","Padding batch  13\n","370\n","Done padding batch  13\n","Padding batch  14\n","342\n","Done padding batch  14\n","Padding batch  15\n","350\n","Done padding batch  15\n","\n","Done.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Pvd2qvUULcdq"},"source":["# test_ids = np.loadtxt(os.getcwd() + '/encoded_tweets.txt', delimiter=', ', ndmin=1, dtype=object)\n","with open(os.getcwd() + '/encoded_tweets.txt', mode='r') as in_file:\n","  test_ids = []\n","  batch_size = 2000000\n","  j = 6 # number of times ive run it, needs to be 6 in end\n","  for i, line in enumerate(in_file):\n","    if i >= j*batch_size and i < (j+1)*batch_size: \n","      l = line.replace('[', '').replace(']','').split(', ')\n","      k = [int(s.strip()) for s in l]\n","      test_ids.append(k)\n","    elif i >= (j+1)*batch_size: break\n","in_file.close()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5i3kOslILsnU","executionInfo":{"elapsed":668390,"status":"ok","timestamp":1614268443290,"user":{"displayName":"Sydney Lister","photoUrl":"","userId":"17557786652639167110"},"user_tz":300},"outputId":"d69a0c3d-64df-4c60-cc62-1e398eb45cd7"},"source":["# Set the maximum sequence length.\n","# I've chosen 400 somewhat arbitrarily. It's slightly larger than the\n","# maximum training tweet length of 357...\n","MAX_LEN = 400\n","\n","print('\\nPadding/truncating all sentences to %d values...' % MAX_LEN)\n","\n","print('\\nPadding token: \"{:}\", ID: {:}'.format(tokenizer.pad_token, tokenizer.pad_token_id))\n","\n","# Pad our input tokens with value 0.\n","# \"post\" indicates that we want to pad and truncate at the end of the sequence,\n","# as opposed to the beginning.\n","# test_ids = pad_sequences(test_ids, maxlen=MAX_LEN, dtype=\"long\", \n","#                           value=0, truncating=\"post\", padding=\"post\")\n","with open(os.getcwd() + '/padded_tweets.txt', mode='a') as out_file: \n","  for i in range(16):\n","    print('Padding batch ', i)\n","    if len(test_ids[i*size:(i+1)*size]) == 0: break \n","    dynamic_padding = len(max(test_ids[i*size:(i+1)*size], key=len)) # https://towardsdatascience.com/divide-hugging-face-transformers-training-time-by-2-or-more-21bf7129db9q-21bf7129db9e\n","    if dynamic_padding > MAX_LEN: \n","      dynamic_padding = MAX_LEN\n","    padded_test_ids = pad_sequences(test_ids[i*size:(i+1)*size], maxlen=dynamic_padding, dtype=\"long\",  value=0, truncating=\"post\", padding=\"post\")\n","    out_file.write('\\n'.join(str(i) for i in padded_test_ids))\n","    print('Done padding batch ', i)\n","\n","  # test_ids_1 = pad_sequences(test_ids[:size], maxlen=MAX_LEN, dtype=\"long\",  value=0, truncating=\"post\", padding=\"post\")\n","  print('\\nDone.')\n","out_file.close()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\n","Padding/truncating all sentences to 400 values...\n","\n","Padding token: \"[PAD]\", ID: 0\n","Padding batch  0\n","Done padding batch  0\n","Padding batch  1\n","Done padding batch  1\n","Padding batch  2\n","Done padding batch  2\n","Padding batch  3\n","Done padding batch  3\n","Padding batch  4\n","Done padding batch  4\n","Padding batch  5\n","Done padding batch  5\n","Padding batch  6\n","Done padding batch  6\n","Padding batch  7\n","Done padding batch  7\n","Padding batch  8\n","Done padding batch  8\n","Padding batch  9\n","Done padding batch  9\n","Padding batch  10\n","Done padding batch  10\n","Padding batch  11\n","\n","Done.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"P51KIncdor4T"},"source":["creating attn masks "]},{"cell_type":"markdown","metadata":{"id":"W80-yS6c7Idi"},"source":["Once the file: os.getcwd() + '/att_mask_final_.txt' has been created...\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b2yyUdvpqyKH","executionInfo":{"status":"ok","timestamp":1619467633045,"user_tz":240,"elapsed":412917,"user":{"displayName":"Sydney Lister","photoUrl":"","userId":"17557786652639167110"}},"outputId":"88626cdc-337f-4262-9a2a-30a731438cca"},"source":["# 1\n","with open(os.getcwd() + '/padded_tweets.txt', mode='r') as in_file:\n","  padded_test_ids = []\n","  batch_size = 45884941\n","  j = 10 # number of times ive run it, needs to be 10 in end\n","  i = 0 \n","  for line in in_file:\n","    if i >= j*batch_size and i < (j+1)*batch_size: \n","      if i == j*batch_size: # start padded sequence\n","        long_line = []\n","        len_long_line = 0\n","      l = [int(x) for x in line.split(' ') if (not len(x) == 0) and x.isnumeric()] \n","      if len_long_line + len(l) <= 400: \n","        long_line.extend(l)\n","        len_long_line += len(l)\n","      else: # end padded_sequence \n","        # print(long_line)\n","        padded_test_ids.append(long_line)\n","        long_line = []\n","        len_long_line = 0\n","    elif i >= (j+1)*batch_size: break\n","    i+= 1\n","in_file.close()\n","print(len(padded_test_ids)) # REMEMBER THIS NUMBER"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"1B5oMbwJq1Uk"},"source":["# 0 - This must happen before going onto step 1 and 2 but step 1 must have been executed in this runtime to run this\n","with open(os.getcwd() + '/att_mask_final_.txt', mode='w') as out_file:\n","# For each tweet... \n","  for tweet in padded_test_ids: \n","      # Create the attention mask.\n","      #   - If a token ID is 0, then it's padding, set the mask to 0.\n","      #   - If a token ID is > 0, then it's a real token, set the mask to 1.\n","      att_mask = [int(token_id > 0) for token_id in tweet]\n","      out_file.write(str(att_mask) + '\\n')\n","out_file.close()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kYJay7EXq6_s","executionInfo":{"status":"ok","timestamp":1619444593343,"user_tz":240,"elapsed":117723,"user":{"displayName":"Sydney Lister","photoUrl":"","userId":"17557786652639167110"}},"outputId":"d5566298-f281-416e-c23e-15567fd75c93"},"source":["# RESTART RUNTIME THEN RUN\n","# 4\n","with open(os.getcwd() + '/att_mask_final_.txt', mode='r') as in_file:\n","  batch_size = 1211972 # SHOULD BE THE NUMBER FROM #1...\n","  att_masks =[]\n","  for i, line in enumerate(in_file):\n","    if i < batch_size:\n","      att_mask = [int(x) for x in line.replace('[','').replace(']','').split(', ')]\n","      att_masks.append(att_mask)\n","    elif i >= batch_size: break\n","in_file.close()\n","print(len(att_masks)) # REMEMBER THIS NUMBER"],"execution_count":null,"outputs":[{"output_type":"stream","text":["1211972\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"G6FLfu7Qq9k9","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"error","timestamp":1619467633411,"user_tz":240,"elapsed":407900,"user":{"displayName":"Sydney Lister","photoUrl":"","userId":"17557786652639167110"}},"outputId":"8e968b06-f9b1-4c07-848a-7399a546defe"},"source":["# 2 \n","length = max(map(len, padded_test_ids)) \n","prediction_inputs = torch.tensor([xi+[0]*(length-len(xi)) for xi in padded_test_ids])"],"execution_count":null,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-9-a97368d8a840>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlength\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadded_test_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprediction_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mxi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mxi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpadded_test_ids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: max() arg is an empty sequence"]}]},{"cell_type":"code","metadata":{"id":"H5G8Jej2jIaG"},"source":["# 3\n","torch.save(prediction_inputs, os.getcwd() + '/inputs.pt')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"O2248Pk178-k"},"source":["If the number printed by #1 is greater than the number printed by #4, run #5a otherwise run #5b "]},{"cell_type":"code","metadata":{"id":"I0QO71ga3m2f"},"source":["#5a \n","length_att = max(map(len, att_masks))\n","mask_list = [xi+[0]*(length_att-len(xi)) for xi in att_masks]\n","extra = [0]*length_att\n","diff_inputs = 1387681 - 1385856\n","# prediction_masks = torch.tensor([xi+[0]*(length_att-len(xi)) for xi in att_masks])\n","mask_list.extend([extra]*diff_inputs)\n","prediction_masks= torch.tensor(mask_list)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lryYNNJT9BOM"},"source":["#5b \n","length_att = max(map(len, att_masks))\n","prediction_masks = torch.tensor([xi+[0]*(length_att-len(xi)) for xi in att_masks])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"onnV1kx8njKi"},"source":["#6\n","torch.save(prediction_masks, os.getcwd() + '/masks.pt')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8j0NX8T2tfLz"},"source":["#RESTART RUNTIME THEN RUN\n","#7\n","torch.cuda.empty_cache()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AWmuWZz5qnAQ"},"source":["#8 \n","prediction_inputs = torch.load(os.getcwd() + '/inputs.pt', torch.device(\"cuda\")) \n","prediction_masks = torch.load(os.getcwd() + '/masks.pt', torch.device(\"cuda\"))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EI5qKGqGyNZi"},"source":["#9\n","batch_size = 16\n","\n","# Create the DataLoader for our prediction set.\n","pred_data = TensorDataset(prediction_inputs, prediction_masks)\n","pred_sampler = RandomSampler(pred_data)\n","pred_dataloader = DataLoader(pred_data, sampler=pred_sampler, batch_size=batch_size)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"L69yjrNTlDeE"},"source":["#10 \n","predictions = []\n","model = model.to(torch.device(\"cuda\"))\n","# Evaluate data for one epoch\n","for batch in pred_dataloader:\n","        \n","        # Add batch to GPU\n","        batch = tuple(t.to(device) for t in batch)\n","        \n","        # Unpack the inputs from our dataloader\n","        b_input_ids, b_input_mask = batch\n","        \n","        # Telling the model not to compute or store gradients, saving memory and\n","        # speeding up validation\n","        with torch.no_grad():        \n","\n","            # Forward pass, calculate logit predictions.\n","            # This will return the logits rather than the loss because we have\n","            # not provided labels.\n","            # token_type_ids is the same as the \"segment ids\", which \n","            # differentiates sentence 1 and 2 in 2-sentence tasks.\n","            # The documentation for this `model` function is here: \n","            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n","            outputs = model(b_input_ids, \n","                            token_type_ids=None, \n","                            attention_mask=b_input_mask)\n","        \n","        # Get the \"logits\" output by the model. The \"logits\" are the output\n","        # values prior to applying an activation function like the softmax.\n","        logits = outputs[0]\n","        # Move logits to CPU\n","        logits = logits.detach().cpu().numpy()\n","        predictions.append(logits)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"weiGwvhKlDBK"},"source":["#11\n","j = '10' # FROM CELL 1\n","with open(os.getcwd() + '/outputs'+ j + '.txt', mode='w') as out_file: \n","  for p in predictions: \n","    out_file.write(str(p) + '\\n')\n","out_file.close()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZdX08qzE0kzp"},"source":["# TRY COMPARING TO DUMMY CLASSIFIER WITH STRATIFIED STRATEGY \n","# CREATE PRECISION RECALL CURVE FOR THESIS "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JJ-9uEEcGSoS"},"source":["all_out = []\n","for i in range(10):\n","  with open(os.getcwd() + '/outputs' + str(i) + '.txt', mode='r') as in_file:\n","    for i, line in enumerate(in_file):\n","      prob = np.fromstring(line.replace('[', '').replace(']',''), dtype=float, sep=' ')\n","      label = np.argmax(prob)\n","      all_out.append(label)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jFr3eFqmNMXa","executionInfo":{"status":"ok","timestamp":1619625830495,"user_tz":240,"elapsed":676,"user":{"displayName":"Sydney Lister","photoUrl":"","userId":"17557786652639167110"}},"outputId":"88fa93ac-b30a-4f50-80d8-fd1b6d159858"},"source":["print(len(all_out))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["12566953\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FjkwYmSEO4hM","executionInfo":{"status":"ok","timestamp":1619626015811,"user_tz":240,"elapsed":1146,"user":{"displayName":"Sydney Lister","photoUrl":"","userId":"17557786652639167110"}},"outputId":"15bed44e-38fa-46d7-92eb-5fc8aa4855d7"},"source":["13304189 - 12566953"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["737236"]},"metadata":{"tags":[]},"execution_count":19}]},{"cell_type":"code","metadata":{"id":"uNX1wwrtPUfP"},"source":["model = BertForSequenceClassification.from_pretrained(os.getcwd() + '/Model/', \n","    num_labels = 2, # The number of output labels--2 for binary classification.   \n","    output_attentions = False, # Whether the model returns attentions weights.\n","    output_hidden_states = True, # Whether the model returns all hidden-states.\n","    ) # Assumes already in Data folder \n","\n","tokenizer = BertTokenizer.from_pretrained(os.getcwd() + '/Model/', do_lower_case=True)\n","\n","def prep_data(input_ids):\n","    # tokenize to get input IDs and attention mask tensors\n","    # tokens = tokenizer.encode_plus(text, max_length=400,\n","    #                                truncation=True, padding='max_length',\n","    #                                add_special_tokens=True, return_token_type_ids=False,\n","    #                                return_tensors='tf')\n","    # # tokenizer returns int32 tensors, we need to return float64, so we use tf.cast\n","    # return {'input_ids': tf.cast(tokens['input_ids'], tf.float64),\n","    #         'attention_mask': tf.cast(tokens['attention_mask'], tf.float64)}\n","\n","    # tokens = tokenizer.encode(\n","    #                     text,                      # Tweet to encode.\n","    #                     add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n","\n","    #                     # This function also supports truncation and conversion\n","    #                     # to pytorch tensors, but we need to do padding, so we\n","    #                     # can't use these features :( .\n","    #                     #max_length = 128,          # Truncate all tweets.\n","    #                     #return_tensors = 'pt',     # Return pytorch tensors.\n","    #                )\n","    input_ids = pad_sequences(input_ids, maxlen=400, dtype=\"long\", \n","                          value=0, truncating=\"post\", padding=\"post\")\n","    att_masks = []\n","    for input_id in input_ids:\n","      att_mask = [int(token_id > 0) for token_id in input_id]\n","      att_masks.append(att_mask)\n","\n","    return input_ids, att_masks"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_tmhmQEIm5cz"},"source":["session.close()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3TTbYZbRwofa"},"source":["def predict(model, pred_dataloader):\n","  predictions = []\n","  model = model.to(torch.device(\"cuda\"))\n","  # Evaluate data for one epoch\n","  for batch in pred_dataloader:\n","          \n","          # Add batch to GPU\n","          batch = tuple(t.to(device) for t in batch)\n","          \n","          # Unpack the inputs from our dataloader\n","          b_input_ids, b_input_mask = batch\n","          \n","          # Telling the model not to compute or store gradients, saving memory and\n","          # speeding up validation\n","          with torch.no_grad():        \n","\n","              # Forward pass, calculate logit predictions.\n","              # This will return the logits rather than the loss because we have\n","              # not provided labels.\n","              # token_type_ids is the same as the \"segment ids\", which \n","              # differentiates sentence 1 and 2 in 2-sentence tasks.\n","              # The documentation for this `model` function is here: \n","              # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n","              outputs = model(b_input_ids, \n","                              token_type_ids=None, \n","                              attention_mask=b_input_mask)\n","          \n","          # Get the \"logits\" output by the model. The \"logits\" are the output\n","          # values prior to applying an activation function like the softmax.\n","          logits = outputs[0]\n","          # Move logits to CPU\n","          logits = logits.detach().cpu().numpy()\n","          predictions.append(logits)\n","  return predictions "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DkPfHmGMPoJg","executionInfo":{"status":"ok","timestamp":1620085668129,"user_tz":240,"elapsed":5482156,"user":{"displayName":"Sydney Lister","photoUrl":"","userId":"17557786652639167110"}},"outputId":"db9bd3a3-6713-459c-b185-9c480495c98d"},"source":["in_tensors = []\n","\n","gpu_options = tf.compat.v1.GPUOptions(allow_growth=True)\n","session = tf.compat.v1.InteractiveSession(config=tf.compat.v1.ConfigProto(gpu_options=gpu_options))\n","\n","print(df.week.unique())\n","\n","remaining = [37., 36.]\n","\n","# for w in df.week.unique():\n","for w in remaining:\n","  print(w)\n","  input_ids = []\n","  att_masks = []\n","  week_text = df[df.week == w]['text']\n","  encoded = []\n","  for i, tweet in week_text.iteritems():\n","\n","    tokens = tokenizer.encode(\n","                        tweet,                      # Tweet to encode.\n","                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n","\n","                        # This function also supports truncation and conversion\n","                        # to pytorch tensors, but we need to do padding, so we\n","                        # can't use these features :( .\n","                        #max_length = 128,          # Truncate all tweets.\n","                        #return_tensors = 'pt',     # Return pytorch tensors.\n","                   )\n","    encoded.append(tokens)\n","  input_ids, att_masks = prep_data(encoded)\n","  \n","  batch_size = 16\n","  if len(input_ids) > 0:\n","    # Create the DataLoader for our prediction set.\n","    pred_data = TensorDataset(torch.tensor(input_ids),torch.tensor(att_masks))\n","    pred_sampler = RandomSampler(pred_data)\n","    pred_dataloader = DataLoader(pred_data, sampler=pred_sampler, batch_size=batch_size)\n","\n","    # Predict on this week's tweets and write their labels to text fil e\n","    predictions = predict(model, pred_dataloader)\n","    with open('predictions_week' + str(w) + '.txt', 'w') as filehandle:\n","      filehandle.writelines(\"%s\\n\" % pred for pred in predictions)\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nan  9.  8.  6.  5.  7. 12. 10. 13. 14. 11. 15. 16. 17. 18. 20. 19. 22.\n"," 21. 23. 26. 30. 29. 27. 28. 31. 34. 33. 35. 32. 38. 37. 36.]\n","37.0\n","36.0\n"],"name":"stdout"}]}]}